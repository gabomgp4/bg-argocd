// *** WARNING: this file was generated by crd2pulumi. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

import * as pulumi from "@pulumi/pulumi";
import * as inputs from "../types/input";
import * as outputs from "../types/output";

import * as utilities from "../utilities";

import {ObjectMeta} from "../meta/v1";

export namespace clickhouse {
    export namespace v1 {
        /**
         * Specification of the desired behavior of one or more ClickHouse clusters
         * More info: https://github.com/Altinity/clickhouse-operator/blob/master/docs/custom_resource_explained.md
         */
        export interface ClickHouseInstallationSpecArgs {
            /**
             * allows configure multiple aspects and behavior for `clickhouse-server` instance and also allows describe multiple `clickhouse-server` clusters inside one `chi` resource
             */
            configuration?: pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationSpecConfigurationArgs>;
            /**
             * define default behavior for whole ClickHouseInstallation, some behavior can be re-define on cluster, shard and replica level
             * More info: https://github.com/Altinity/clickhouse-operator/blob/master/docs/custom_resource_explained.md#specdefaults
             */
            defaults?: pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationSpecDefaultsArgs>;
            /**
             * custom domain suffix which will add to end of `Service` or `Pod` name, use it when you use custom cluster domain in your Kubernetes cluster
             */
            namespaceDomainPattern?: pulumi.Input<string>;
            /**
             * optional, allows tuning reconciling cycle for ClickhouseInstallation from clickhouse-operator side
             */
            reconciling?: pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationSpecReconcilingArgs>;
            /**
             * This is a 'soft restart' button. When set to 'RollingUpdate' operator will restart ClickHouse pods in a graceful way. Remove it after the use in order to avoid unneeded restarts
             */
            restart?: pulumi.Input<string>;
            /**
             * Allow stop all ClickHouse clusters described in current chi.
             * Stop mechanism works as follows:
             *  - When `stop` is `1` then setup `Replicas: 0` in each related to current `chi` StatefulSet resource, all `Pods` and `Service` resources will desctroy, but PVCs still live
             *  - When `stop` is `0` then `Pods` will created again and will attach retained PVCs and `Service` also will created again
             */
            stop?: pulumi.Input<string>;
            /**
             * Allows to define custom taskID for named update operation and watch status of this update execution in .status.taskIDs field.
             * By default every update of chi manifest will generate random taskID
             */
            taskID?: pulumi.Input<string>;
            /**
             * allows define templates which will use for render Kubernetes resources like StatefulSet, ConfigMap, Service, PVC, by default, clickhouse-operator have own templates, but you can override it
             */
            templates?: pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationSpecTemplatesArgs>;
            /**
             * optional, define policy for auto applying ClickHouseInstallationTemplate inside ClickHouseInstallation
             */
            templating?: pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationSpecTemplatingArgs>;
            /**
             * allows troubleshoot Pods during CrashLoopBack state, when you apply wrong configuration, `clickhouse-server` wouldn't startup
             */
            troubleshoot?: pulumi.Input<string>;
            /**
             * list of `ClickHouseInstallationTemplate` (chit) resource names which will merge with current `Chi` manifest during render Kubernetes resources to create related ClickHouse clusters
             */
            useTemplates?: pulumi.Input<pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationSpecUsetemplatesArgs>[]>;
        }

        /**
         * allows configure multiple aspects and behavior for `clickhouse-server` instance and also allows describe multiple `clickhouse-server` clusters inside one `chi` resource
         */
        export interface ClickHouseInstallationSpecConfigurationArgs {
            /**
             * describes ClickHouse clusters layout and allows change settings on cluster-level, shard-level and replica-level
             * every cluster is a set of StatefulSet, one StatefulSet contains only one Pod with `clickhouse-server`
             * all Pods will rendered in <remote_server> part of ClickHouse configs, mounted from ConfigMap as `/etc/clickhouse-server/config.d/chop-generated-remote_servers.xml`
             * Clusters will use for Distributed table engine, more details: https://clickhouse.tech/docs/en/engines/table-engines/special/distributed/
             * If `cluster` contains zookeeper settings (could be inherited from top `chi` level), when you can create *ReplicatedMergeTree tables
             */
            clusters?: pulumi.Input<pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationSpecConfigurationClustersArgs>[]>;
            /**
             * allows define content of any setting file inside each `Pod` during generate `ConfigMap` which will mount in `/etc/clickhouse-server/config.d/` or `/etc/clickhouse-server/conf.d/` or `/etc/clickhouse-server/users.d/`
             * every key in this object is the file name
             * every value in this object is the file content
             * you can use `!!binary |` and base64 for binary files, see details here https://yaml.org/type/binary.html
             * each key could contains prefix like USERS, COMMON, HOST or config.d, users.d, cond.d, wrong prefixes will ignored, subfolders also will ignored
             * More details: https://github.com/Altinity/clickhouse-operator/blob/master/docs/chi-examples/05-settings-05-files-nested.yaml
             */
            files?: pulumi.Input<{[key: string]: any}>;
            /**
             * allows configure <yandex><profiles>..</profiles></yandex> section in each `Pod` during generate `ConfigMap` which will mount in `/etc/clickhouse-server/users.d/`
             * you can configure any aspect of settings profile
             * More details: https://clickhouse.tech/docs/en/operations/settings/settings-profiles/
             * Your yaml code will convert to XML, see examples https://github.com/Altinity/clickhouse-operator/blob/master/docs/custom_resource_explained.md#specconfigurationprofiles
             */
            profiles?: pulumi.Input<{[key: string]: any}>;
            /**
             * allows configure <yandex><quotas>..</quotas></yandex> section in each `Pod` during generate `ConfigMap` which will mount in `/etc/clickhouse-server/users.d/`
             * you can configure any aspect of resource quotas
             * More details: https://clickhouse.tech/docs/en/operations/quotas/
             * Your yaml code will convert to XML, see examples https://github.com/Altinity/clickhouse-operator/blob/master/docs/custom_resource_explained.md#specconfigurationquotas
             */
            quotas?: pulumi.Input<{[key: string]: any}>;
            /**
             * allows configure `clickhouse-server` settings inside <yandex>...</yandex> tag in each `Pod` during generate `ConfigMap` which will mount in `/etc/clickhouse-server/config.d/`
             * More details: https://clickhouse.tech/docs/en/operations/settings/settings/
             * Your yaml code will convert to XML, see examples https://github.com/Altinity/clickhouse-operator/blob/master/docs/custom_resource_explained.md#specconfigurationsettings
             */
            settings?: pulumi.Input<{[key: string]: any}>;
            /**
             * allows configure <yandex><users>..</users></yandex> section in each `Pod` during generate `ConfigMap` which will mount in `/etc/clickhouse-server/users.d/`
             * you can configure password hashed, authorization restrictions, database level security row filters etc.
             * More details: https://clickhouse.tech/docs/en/operations/settings/settings-users/
             * Your yaml code will convert to XML, see examples https://github.com/Altinity/clickhouse-operator/blob/master/docs/custom_resource_explained.md#specconfigurationusers
             */
            users?: pulumi.Input<{[key: string]: any}>;
            /**
             * allows configure <yandex><zookeeper>..</zookeeper></yandex> section in each `Pod` during generate `ConfigMap` which will mounted in `/etc/clickhouse-server/config.d/`
             * `clickhouse-operator` itself doesn't manage Zookeeper, please install Zookeeper separatelly look examples on https://github.com/Altinity/clickhouse-operator/tree/master/deploy/zookeeper/
             * currently, zookeeper (or clickhouse-keeper replacement) used for *ReplicatedMergeTree table engines and for `distributed_ddl`
             * More details: https://clickhouse.tech/docs/en/operations/server-configuration-parameters/settings/#server-settings_zookeeper
             */
            zookeeper?: pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationSpecConfigurationZookeeperArgs>;
        }

        export interface ClickHouseInstallationSpecConfigurationClustersArgs {
            /**
             * optional, allows define content of any setting file inside each `Pod` on current cluster during generate `ConfigMap` which will mount in `/etc/clickhouse-server/config.d/` or `/etc/clickhouse-server/conf.d/` or `/etc/clickhouse-server/users.d/`
             * override top-level `chi.spec.configuration.files`
             */
            files?: pulumi.Input<{[key: string]: any}>;
            /**
             * optional, open insecure ports for cluster, defaults to "yes"
             */
            insecure?: pulumi.Input<string>;
            /**
             * describe current cluster layout, how much shards in cluster, how much replica in shard
             * allows override settings on each shard and replica separatelly
             */
            layout?: pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationSpecConfigurationClustersLayoutArgs>;
            /**
             * cluster name, used to identify set of ClickHouse servers and wide used during generate names of related Kubernetes resources
             */
            name?: pulumi.Input<string>;
            /**
             * describes how schema is propagated within replicas and shards
             */
            schemaPolicy?: pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationSpecConfigurationClustersSchemapolicyArgs>;
            /**
             * optional, shared secret value to secure cluster communications
             */
            secret?: pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationSpecConfigurationClustersSecretArgs>;
            /**
             * optional, open secure ports for cluster
             */
            secure?: pulumi.Input<string>;
            /**
             * optional, allows configure `clickhouse-server` settings inside <yandex>...</yandex> tag in each `Pod` only in one cluster during generate `ConfigMap` which will mount in `/etc/clickhouse-server/config.d/`
             * override top-level `chi.spec.configuration.settings`
             * More details: https://clickhouse.tech/docs/en/operations/settings/settings/
             */
            settings?: pulumi.Input<{[key: string]: any}>;
            /**
             * optional, configuration of the templates names which will use for generate Kubernetes resources according to selected cluster
             * override top-level `chi.spec.configuration.templates`
             */
            templates?: pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationSpecConfigurationClustersTemplatesArgs>;
            /**
             * optional, allows configure <yandex><zookeeper>..</zookeeper></yandex> section in each `Pod` only in current ClickHouse cluster, during generate `ConfigMap` which will mounted in `/etc/clickhouse-server/config.d/`
             * override top-level `chi.spec.configuration.zookeeper` settings
             */
            zookeeper?: pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationSpecConfigurationClustersZookeeperArgs>;
        }

        /**
         * describe current cluster layout, how much shards in cluster, how much replica in shard
         * allows override settings on each shard and replica separatelly
         */
        export interface ClickHouseInstallationSpecConfigurationClustersLayoutArgs {
            /**
             * optional, allows override top-level `chi.spec.configuration` and cluster-level `chi.spec.configuration.clusters` configuration for each replica and each shard relates to selected replica, use it only if you fully understand what you do
             */
            replicas?: pulumi.Input<pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationSpecConfigurationClustersLayoutReplicasArgs>[]>;
            /**
             * how much replicas in each shards for current ClickHouse cluster will run in Kubernetes, each replica is a separate `StatefulSet` which contains only one `Pod` with `clickhouse-server` instance, every shard contains 1 replica by default
             */
            replicasCount?: pulumi.Input<number>;
            /**
             * optional, allows override top-level `chi.spec.configuration`, cluster-level `chi.spec.configuration.clusters` settings for each shard separately, use it only if you fully understand what you do
             */
            shards?: pulumi.Input<pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationSpecConfigurationClustersLayoutShardsArgs>[]>;
            /**
             * how much shards for current ClickHouse cluster will run in Kubernetes, each shard contains shared-nothing part of data and contains set of replicas, cluster contains 1 shard by default
             */
            shardsCount?: pulumi.Input<number>;
            /**
             * DEPRECATED - to be removed soon
             */
            type?: pulumi.Input<string>;
        }

        export interface ClickHouseInstallationSpecConfigurationClustersLayoutReplicasArgs {
            /**
             * optional, allows define content of any setting file inside each `Pod` only in one replica during generate `ConfigMap` which will mount in `/etc/clickhouse-server/config.d/` or `/etc/clickhouse-server/conf.d/` or `/etc/clickhouse-server/users.d/`
             * override top-level `chi.spec.configuration.files` and cluster-level `chi.spec.configuration.clusters.files`, will ignore if `chi.spec.configuration.clusters.layout.shards` presents
             */
            files?: pulumi.Input<{[key: string]: any}>;
            /**
             * optional, by default replica name is generated, but you can override it and setup custom name
             */
            name?: pulumi.Input<string>;
            /**
             * optional, allows configure `clickhouse-server` settings inside <yandex>...</yandex> tag in `Pod` only in one replica during generate `ConfigMap` which will mount in `/etc/clickhouse-server/conf.d/`
             * override top-level `chi.spec.configuration.settings`, cluster-level `chi.spec.configuration.clusters.settings` and will ignore if shard-level `chi.spec.configuration.clusters.layout.shards` present
             * More details: https://clickhouse.tech/docs/en/operations/settings/settings/
             */
            settings?: pulumi.Input<{[key: string]: any}>;
            /**
             * optional, list of shards related to current replica, will ignore if `chi.spec.configuration.clusters.layout.shards` presents
             */
            shards?: pulumi.Input<pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationSpecConfigurationClustersLayoutReplicasShardsArgs>[]>;
            /**
             * optional, count of shards related to current replica, you can override each shard behavior on low-level `chi.spec.configuration.clusters.layout.replicas.shards`
             */
            shardsCount?: pulumi.Input<number>;
            /**
             * optional, configuration of the templates names which will use for generate Kubernetes resources according to selected replica
             * override top-level `chi.spec.configuration.templates`, cluster-level `chi.spec.configuration.clusters.templates`
             */
            templates?: pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationSpecConfigurationClustersLayoutReplicasTemplatesArgs>;
        }

        export interface ClickHouseInstallationSpecConfigurationClustersLayoutReplicasShardsArgs {
            /**
             * optional, allows define content of any setting file inside each `Pod` only in one shard related to current replica during generate `ConfigMap` which will mount in `/etc/clickhouse-server/config.d/` or `/etc/clickhouse-server/conf.d/` or `/etc/clickhouse-server/users.d/`
             * override top-level `chi.spec.configuration.files` and cluster-level `chi.spec.configuration.clusters.files`, will ignore if `chi.spec.configuration.clusters.layout.shards` presents
             */
            files?: pulumi.Input<{[key: string]: any}>;
            /**
             * optional, setup `Pod.spec.containers.ports` with name `http` for selected shard, override `chi.spec.templates.hostTemplates.spec.httpPort`
             * allows connect to `clickhouse-server` via HTTP protocol via kubernetes `Service`
             */
            httpPort?: pulumi.Input<number>;
            httpsPort?: pulumi.Input<number>;
            /**
             * optional, open insecure ports for cluster, defaults to "yes"
             */
            insecure?: pulumi.Input<string>;
            /**
             * optional, setup `Pod.spec.containers.ports` with name `interserver` for selected shard, override `chi.spec.templates.hostTemplates.spec.interserverHTTPPort`
             * allows connect between replicas inside same shard during fetch replicated data parts HTTP protocol
             */
            interserverHTTPPort?: pulumi.Input<number>;
            /**
             * optional, by default shard name is generated, but you can override it and setup custom name
             */
            name?: pulumi.Input<string>;
            /**
             * optional, open secure ports
             */
            secure?: pulumi.Input<string>;
            /**
             * optional, allows configure `clickhouse-server` settings inside <yandex>...</yandex> tag in `Pod` only in one shard related to current replica during generate `ConfigMap` which will mount in `/etc/clickhouse-server/conf.d/`
             * override top-level `chi.spec.configuration.settings`, cluster-level `chi.spec.configuration.clusters.settings` and replica-level `chi.spec.configuration.clusters.layout.replicas.settings`
             * More details: https://clickhouse.tech/docs/en/operations/settings/settings/
             */
            settings?: pulumi.Input<{[key: string]: any}>;
            /**
             * optional, setup `Pod.spec.containers.ports` with name `tcp` for selected shard, override `chi.spec.templates.hostTemplates.spec.tcpPort`
             * allows connect to `clickhouse-server` via TCP Native protocol via kubernetes `Service`
             */
            tcpPort?: pulumi.Input<number>;
            /**
             * optional, configuration of the templates names which will use for generate Kubernetes resources according to selected replica
             * override top-level `chi.spec.configuration.templates`, cluster-level `chi.spec.configuration.clusters.templates`, replica-level `chi.spec.configuration.clusters.layout.replicas.templates`
             */
            templates?: pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationSpecConfigurationClustersLayoutReplicasShardsTemplatesArgs>;
            tlsPort?: pulumi.Input<number>;
        }

        /**
         * optional, configuration of the templates names which will use for generate Kubernetes resources according to selected replica
         * override top-level `chi.spec.configuration.templates`, cluster-level `chi.spec.configuration.clusters.templates`, replica-level `chi.spec.configuration.clusters.layout.replicas.templates`
         */
        export interface ClickHouseInstallationSpecConfigurationClustersLayoutReplicasShardsTemplatesArgs {
            /**
             * optional, template name from chi.spec.templates.serviceTemplates, allows customization for each `Service` resource which will created by `clickhouse-operator` which cover each clickhouse cluster described in `chi.spec.configuration.clusters`
             */
            clusterServiceTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.volumeClaimTemplates, allows customization each `PVC` which will mount for clickhouse data directory in each `Pod` during render and reconcile every StatefulSet.spec resource described in `chi.spec.configuration.clusters`
             */
            dataVolumeClaimTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.hostTemplates, which will apply to configure every `clickhouse-server` instance during render ConfigMap resources which will mount into `Pod`
             */
            hostTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.volumeClaimTemplates, allows customization each `PVC` which will mount for clickhouse log directory in each `Pod` during render and reconcile every StatefulSet.spec resource described in `chi.spec.configuration.clusters`
             */
            logVolumeClaimTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.podTemplates, allows customization each `Pod` resource during render and reconcile each StatefulSet.spec resource described in `chi.spec.configuration.clusters`
             */
            podTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.serviceTemplates, allows customization for each `Service` resource which will created by `clickhouse-operator` which cover each replica inside each shard inside each clickhouse cluster described in `chi.spec.configuration.clusters`
             */
            replicaServiceTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.serviceTemplates, allows customization for one `Service` resource which will created by `clickhouse-operator` which cover all clusters in whole `chi` resource
             */
            serviceTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.serviceTemplates, allows customization for each `Service` resource which will created by `clickhouse-operator` which cover each shard inside clickhouse cluster described in `chi.spec.configuration.clusters`
             */
            shardServiceTemplate?: pulumi.Input<string>;
            /**
             * DEPRECATED! VolumeClaimTemplate is deprecated in favor of DataVolumeClaimTemplate and LogVolumeClaimTemplate
             */
            volumeClaimTemplate?: pulumi.Input<string>;
        }

        /**
         * optional, configuration of the templates names which will use for generate Kubernetes resources according to selected replica
         * override top-level `chi.spec.configuration.templates`, cluster-level `chi.spec.configuration.clusters.templates`
         */
        export interface ClickHouseInstallationSpecConfigurationClustersLayoutReplicasTemplatesArgs {
            /**
             * optional, template name from chi.spec.templates.serviceTemplates, allows customization for each `Service` resource which will created by `clickhouse-operator` which cover each clickhouse cluster described in `chi.spec.configuration.clusters`
             */
            clusterServiceTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.volumeClaimTemplates, allows customization each `PVC` which will mount for clickhouse data directory in each `Pod` during render and reconcile every StatefulSet.spec resource described in `chi.spec.configuration.clusters`
             */
            dataVolumeClaimTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.hostTemplates, which will apply to configure every `clickhouse-server` instance during render ConfigMap resources which will mount into `Pod`
             */
            hostTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.volumeClaimTemplates, allows customization each `PVC` which will mount for clickhouse log directory in each `Pod` during render and reconcile every StatefulSet.spec resource described in `chi.spec.configuration.clusters`
             */
            logVolumeClaimTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.podTemplates, allows customization each `Pod` resource during render and reconcile each StatefulSet.spec resource described in `chi.spec.configuration.clusters`
             */
            podTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.serviceTemplates, allows customization for each `Service` resource which will created by `clickhouse-operator` which cover each replica inside each shard inside each clickhouse cluster described in `chi.spec.configuration.clusters`
             */
            replicaServiceTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.serviceTemplates, allows customization for one `Service` resource which will created by `clickhouse-operator` which cover all clusters in whole `chi` resource
             */
            serviceTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.serviceTemplates, allows customization for each `Service` resource which will created by `clickhouse-operator` which cover each shard inside clickhouse cluster described in `chi.spec.configuration.clusters`
             */
            shardServiceTemplate?: pulumi.Input<string>;
            /**
             * DEPRECATED! VolumeClaimTemplate is deprecated in favor of DataVolumeClaimTemplate and LogVolumeClaimTemplate
             */
            volumeClaimTemplate?: pulumi.Input<string>;
        }

        export interface ClickHouseInstallationSpecConfigurationClustersLayoutShardsArgs {
            /**
             * DEPRECATED - to be removed soon
             */
            definitionType?: pulumi.Input<string>;
            /**
             * optional, allows define content of any setting file inside each `Pod` only in one shard during generate `ConfigMap` which will mount in `/etc/clickhouse-server/config.d/` or `/etc/clickhouse-server/conf.d/` or `/etc/clickhouse-server/users.d/`
             * override top-level `chi.spec.configuration.files` and cluster-level `chi.spec.configuration.clusters.files`
             */
            files?: pulumi.Input<{[key: string]: any}>;
            /**
             * optional, `true` by default when `chi.spec.configuration.clusters[].layout.ReplicaCount` > 1 and 0 otherwise
             * allows setup <internal_replication> setting which will use during insert into tables with `Distributed` engine for insert only in one live replica and other replicas will download inserted data during replication,
             * will apply in <remote_servers> inside ConfigMap which will mount in /etc/clickhouse-server/config.d/chop-generated-remote_servers.xml
             * More details: https://clickhouse.tech/docs/en/engines/table-engines/special/distributed/
             */
            internalReplication?: pulumi.Input<string>;
            /**
             * optional, by default shard name is generated, but you can override it and setup custom name
             */
            name?: pulumi.Input<string>;
            /**
             * optional, allows override behavior for selected replicas from cluster-level `chi.spec.configuration.clusters` and shard-level `chi.spec.configuration.clusters.layout.shards`
             */
            replicas?: pulumi.Input<pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationSpecConfigurationClustersLayoutShardsReplicasArgs>[]>;
            /**
             * optional, how much replicas in selected shard for selected ClickHouse cluster will run in Kubernetes, each replica is a separate `StatefulSet` which contains only one `Pod` with `clickhouse-server` instance,
             * shard contains 1 replica by default
             * override cluster-level `chi.spec.configuration.clusters.layout.replicasCount`
             */
            replicasCount?: pulumi.Input<number>;
            /**
             * optional, allows configure `clickhouse-server` settings inside <yandex>...</yandex> tag in each `Pod` only in one shard during generate `ConfigMap` which will mount in `/etc/clickhouse-server/config.d/`
             * override top-level `chi.spec.configuration.settings` and cluster-level `chi.spec.configuration.clusters.settings`
             * More details: https://clickhouse.tech/docs/en/operations/settings/settings/
             */
            settings?: pulumi.Input<{[key: string]: any}>;
            /**
             * optional, configuration of the templates names which will use for generate Kubernetes resources according to selected shard
             * override top-level `chi.spec.configuration.templates` and cluster-level `chi.spec.configuration.clusters.templates`
             */
            templates?: pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationSpecConfigurationClustersLayoutShardsTemplatesArgs>;
            /**
             * optional, 1 by default, allows setup shard <weight> setting which will use during insert into tables with `Distributed` engine,
             * will apply in <remote_servers> inside ConfigMap which will mount in /etc/clickhouse-server/config.d/chop-generated-remote_servers.xml
             * More details: https://clickhouse.tech/docs/en/engines/table-engines/special/distributed/
             */
            weight?: pulumi.Input<number>;
        }

        export interface ClickHouseInstallationSpecConfigurationClustersLayoutShardsReplicasArgs {
            /**
             * optional, allows define content of any setting file inside `Pod` only in one replica during generate `ConfigMap` which will mount in `/etc/clickhouse-server/config.d/` or `/etc/clickhouse-server/conf.d/` or `/etc/clickhouse-server/users.d/`
             * override top-level `chi.spec.configuration.files`, cluster-level `chi.spec.configuration.clusters.files` and shard-level `chi.spec.configuration.clusters.layout.shards.files`
             */
            files?: pulumi.Input<{[key: string]: any}>;
            /**
             * optional, setup `Pod.spec.containers.ports` with name `http` for selected replica, override `chi.spec.templates.hostTemplates.spec.httpPort`
             * allows connect to `clickhouse-server` via HTTP protocol via kubernetes `Service`
             */
            httpPort?: pulumi.Input<number>;
            httpsPort?: pulumi.Input<number>;
            /**
             * optional, open insecure ports for cluster, defaults to "yes"
             */
            insecure?: pulumi.Input<string>;
            /**
             * optional, setup `Pod.spec.containers.ports` with name `interserver` for selected replica, override `chi.spec.templates.hostTemplates.spec.interserverHTTPPort`
             * allows connect between replicas inside same shard during fetch replicated data parts HTTP protocol
             */
            interserverHTTPPort?: pulumi.Input<number>;
            /**
             * optional, by default replica name is generated, but you can override it and setup custom name
             */
            name?: pulumi.Input<string>;
            /**
             * optional, open secure ports
             */
            secure?: pulumi.Input<string>;
            /**
             * optional, allows configure `clickhouse-server` settings inside <yandex>...</yandex> tag in `Pod` only in one replica during generate `ConfigMap` which will mount in `/etc/clickhouse-server/conf.d/`
             * override top-level `chi.spec.configuration.settings`, cluster-level `chi.spec.configuration.clusters.settings` and shard-level `chi.spec.configuration.clusters.layout.shards.settings`
             * More details: https://clickhouse.tech/docs/en/operations/settings/settings/
             */
            settings?: pulumi.Input<{[key: string]: any}>;
            /**
             * optional, setup `Pod.spec.containers.ports` with name `tcp` for selected replica, override `chi.spec.templates.hostTemplates.spec.tcpPort`
             * allows connect to `clickhouse-server` via TCP Native protocol via kubernetes `Service`
             */
            tcpPort?: pulumi.Input<number>;
            /**
             * optional, configuration of the templates names which will use for generate Kubernetes resources according to selected replica
             * override top-level `chi.spec.configuration.templates`, cluster-level `chi.spec.configuration.clusters.templates` and shard-level `chi.spec.configuration.clusters.layout.shards.templates`
             */
            templates?: pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationSpecConfigurationClustersLayoutShardsReplicasTemplatesArgs>;
            tlsPort?: pulumi.Input<number>;
        }

        /**
         * optional, configuration of the templates names which will use for generate Kubernetes resources according to selected replica
         * override top-level `chi.spec.configuration.templates`, cluster-level `chi.spec.configuration.clusters.templates` and shard-level `chi.spec.configuration.clusters.layout.shards.templates`
         */
        export interface ClickHouseInstallationSpecConfigurationClustersLayoutShardsReplicasTemplatesArgs {
            /**
             * optional, template name from chi.spec.templates.serviceTemplates, allows customization for each `Service` resource which will created by `clickhouse-operator` which cover each clickhouse cluster described in `chi.spec.configuration.clusters`
             */
            clusterServiceTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.volumeClaimTemplates, allows customization each `PVC` which will mount for clickhouse data directory in each `Pod` during render and reconcile every StatefulSet.spec resource described in `chi.spec.configuration.clusters`
             */
            dataVolumeClaimTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.hostTemplates, which will apply to configure every `clickhouse-server` instance during render ConfigMap resources which will mount into `Pod`
             */
            hostTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.volumeClaimTemplates, allows customization each `PVC` which will mount for clickhouse log directory in each `Pod` during render and reconcile every StatefulSet.spec resource described in `chi.spec.configuration.clusters`
             */
            logVolumeClaimTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.podTemplates, allows customization each `Pod` resource during render and reconcile each StatefulSet.spec resource described in `chi.spec.configuration.clusters`
             */
            podTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.serviceTemplates, allows customization for each `Service` resource which will created by `clickhouse-operator` which cover each replica inside each shard inside each clickhouse cluster described in `chi.spec.configuration.clusters`
             */
            replicaServiceTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.serviceTemplates, allows customization for one `Service` resource which will created by `clickhouse-operator` which cover all clusters in whole `chi` resource
             */
            serviceTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.serviceTemplates, allows customization for each `Service` resource which will created by `clickhouse-operator` which cover each shard inside clickhouse cluster described in `chi.spec.configuration.clusters`
             */
            shardServiceTemplate?: pulumi.Input<string>;
            /**
             * DEPRECATED! VolumeClaimTemplate is deprecated in favor of DataVolumeClaimTemplate and LogVolumeClaimTemplate
             */
            volumeClaimTemplate?: pulumi.Input<string>;
        }

        /**
         * optional, configuration of the templates names which will use for generate Kubernetes resources according to selected shard
         * override top-level `chi.spec.configuration.templates` and cluster-level `chi.spec.configuration.clusters.templates`
         */
        export interface ClickHouseInstallationSpecConfigurationClustersLayoutShardsTemplatesArgs {
            /**
             * optional, template name from chi.spec.templates.serviceTemplates, allows customization for each `Service` resource which will created by `clickhouse-operator` which cover each clickhouse cluster described in `chi.spec.configuration.clusters`
             */
            clusterServiceTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.volumeClaimTemplates, allows customization each `PVC` which will mount for clickhouse data directory in each `Pod` during render and reconcile every StatefulSet.spec resource described in `chi.spec.configuration.clusters`
             */
            dataVolumeClaimTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.hostTemplates, which will apply to configure every `clickhouse-server` instance during render ConfigMap resources which will mount into `Pod`
             */
            hostTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.volumeClaimTemplates, allows customization each `PVC` which will mount for clickhouse log directory in each `Pod` during render and reconcile every StatefulSet.spec resource described in `chi.spec.configuration.clusters`
             */
            logVolumeClaimTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.podTemplates, allows customization each `Pod` resource during render and reconcile each StatefulSet.spec resource described in `chi.spec.configuration.clusters`
             */
            podTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.serviceTemplates, allows customization for each `Service` resource which will created by `clickhouse-operator` which cover each replica inside each shard inside each clickhouse cluster described in `chi.spec.configuration.clusters`
             */
            replicaServiceTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.serviceTemplates, allows customization for one `Service` resource which will created by `clickhouse-operator` which cover all clusters in whole `chi` resource
             */
            serviceTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.serviceTemplates, allows customization for each `Service` resource which will created by `clickhouse-operator` which cover each shard inside clickhouse cluster described in `chi.spec.configuration.clusters`
             */
            shardServiceTemplate?: pulumi.Input<string>;
            /**
             * DEPRECATED! VolumeClaimTemplate is deprecated in favor of DataVolumeClaimTemplate and LogVolumeClaimTemplate
             */
            volumeClaimTemplate?: pulumi.Input<string>;
        }

        /**
         * describes how schema is propagated within replicas and shards
         */
        export interface ClickHouseInstallationSpecConfigurationClustersSchemapolicyArgs {
            /**
             * how schema is propagated within a replica
             */
            replica?: pulumi.Input<string>;
            /**
             * how schema is propagated between shards
             */
            shard?: pulumi.Input<string>;
        }

        /**
         * optional, shared secret value to secure cluster communications
         */
        export interface ClickHouseInstallationSpecConfigurationClustersSecretArgs {
            /**
             * Auto-generate shared secret value to secure cluster communications
             */
            auto?: pulumi.Input<string>;
            /**
             * Cluster shared secret value in plain text
             */
            value?: pulumi.Input<string>;
            /**
             * Cluster shared secret source
             */
            valueFrom?: pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationSpecConfigurationClustersSecretValuefromArgs>;
        }

        /**
         * Cluster shared secret source
         */
        export interface ClickHouseInstallationSpecConfigurationClustersSecretValuefromArgs {
            /**
             * Selects a key of a secret in the clickhouse installation namespace.
             * Should not be used if value is not empty.
             */
            secretKeyRef?: pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationSpecConfigurationClustersSecretValuefromSecretkeyrefArgs>;
        }

        /**
         * Selects a key of a secret in the clickhouse installation namespace.
         * Should not be used if value is not empty.
         */
        export interface ClickHouseInstallationSpecConfigurationClustersSecretValuefromSecretkeyrefArgs {
            /**
             * The key of the secret to select from. Must be a valid secret key.
             */
            key: pulumi.Input<string>;
            /**
             * Name of the referent. More info:
             * https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
             */
            name: pulumi.Input<string>;
            /**
             * Specify whether the Secret or its key must be defined
             */
            optional?: pulumi.Input<boolean>;
        }

        /**
         * optional, configuration of the templates names which will use for generate Kubernetes resources according to selected cluster
         * override top-level `chi.spec.configuration.templates`
         */
        export interface ClickHouseInstallationSpecConfigurationClustersTemplatesArgs {
            /**
             * optional, template name from chi.spec.templates.serviceTemplates, allows customization for each `Service` resource which will created by `clickhouse-operator` which cover each clickhouse cluster described in `chi.spec.configuration.clusters`
             */
            clusterServiceTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.volumeClaimTemplates, allows customization each `PVC` which will mount for clickhouse data directory in each `Pod` during render and reconcile every StatefulSet.spec resource described in `chi.spec.configuration.clusters`
             */
            dataVolumeClaimTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.hostTemplates, which will apply to configure every `clickhouse-server` instance during render ConfigMap resources which will mount into `Pod`
             */
            hostTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.volumeClaimTemplates, allows customization each `PVC` which will mount for clickhouse log directory in each `Pod` during render and reconcile every StatefulSet.spec resource described in `chi.spec.configuration.clusters`
             */
            logVolumeClaimTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.podTemplates, allows customization each `Pod` resource during render and reconcile each StatefulSet.spec resource described in `chi.spec.configuration.clusters`
             */
            podTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.serviceTemplates, allows customization for each `Service` resource which will created by `clickhouse-operator` which cover each replica inside each shard inside each clickhouse cluster described in `chi.spec.configuration.clusters`
             */
            replicaServiceTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.serviceTemplates, allows customization for one `Service` resource which will created by `clickhouse-operator` which cover all clusters in whole `chi` resource
             */
            serviceTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.serviceTemplates, allows customization for each `Service` resource which will created by `clickhouse-operator` which cover each shard inside clickhouse cluster described in `chi.spec.configuration.clusters`
             */
            shardServiceTemplate?: pulumi.Input<string>;
            /**
             * DEPRECATED! VolumeClaimTemplate is deprecated in favor of DataVolumeClaimTemplate and LogVolumeClaimTemplate
             */
            volumeClaimTemplate?: pulumi.Input<string>;
        }

        /**
         * optional, allows configure <yandex><zookeeper>..</zookeeper></yandex> section in each `Pod` only in current ClickHouse cluster, during generate `ConfigMap` which will mounted in `/etc/clickhouse-server/config.d/`
         * override top-level `chi.spec.configuration.zookeeper` settings
         */
        export interface ClickHouseInstallationSpecConfigurationClustersZookeeperArgs {
            /**
             * optional access credentials string with `user:password` format used when use digest authorization in Zookeeper
             */
            identity?: pulumi.Input<string>;
            /**
             * describe every available zookeeper cluster node for interaction
             */
            nodes?: pulumi.Input<pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationSpecConfigurationClustersZookeeperNodesArgs>[]>;
            /**
             * one operation timeout during Zookeeper transactions
             */
            operation_timeout_ms?: pulumi.Input<number>;
            /**
             * optional root znode path inside zookeeper to store ClickHouse related data (replication queue or distributed DDL)
             */
            root?: pulumi.Input<string>;
            /**
             * session timeout during connect to Zookeeper
             */
            session_timeout_ms?: pulumi.Input<number>;
        }

        export interface ClickHouseInstallationSpecConfigurationClustersZookeeperNodesArgs {
            /**
             * dns name or ip address for Zookeeper node
             */
            host?: pulumi.Input<string>;
            /**
             * TCP port which used to connect to Zookeeper node
             */
            port?: pulumi.Input<number>;
            /**
             * if a secure connection to Zookeeper is required
             */
            secure?: pulumi.Input<string>;
        }

        /**
         * allows configure <yandex><zookeeper>..</zookeeper></yandex> section in each `Pod` during generate `ConfigMap` which will mounted in `/etc/clickhouse-server/config.d/`
         * `clickhouse-operator` itself doesn't manage Zookeeper, please install Zookeeper separatelly look examples on https://github.com/Altinity/clickhouse-operator/tree/master/deploy/zookeeper/
         * currently, zookeeper (or clickhouse-keeper replacement) used for *ReplicatedMergeTree table engines and for `distributed_ddl`
         * More details: https://clickhouse.tech/docs/en/operations/server-configuration-parameters/settings/#server-settings_zookeeper
         */
        export interface ClickHouseInstallationSpecConfigurationZookeeperArgs {
            /**
             * optional access credentials string with `user:password` format used when use digest authorization in Zookeeper
             */
            identity?: pulumi.Input<string>;
            /**
             * describe every available zookeeper cluster node for interaction
             */
            nodes?: pulumi.Input<pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationSpecConfigurationZookeeperNodesArgs>[]>;
            /**
             * one operation timeout during Zookeeper transactions
             */
            operation_timeout_ms?: pulumi.Input<number>;
            /**
             * optional root znode path inside zookeeper to store ClickHouse related data (replication queue or distributed DDL)
             */
            root?: pulumi.Input<string>;
            /**
             * session timeout during connect to Zookeeper
             */
            session_timeout_ms?: pulumi.Input<number>;
        }

        export interface ClickHouseInstallationSpecConfigurationZookeeperNodesArgs {
            /**
             * dns name or ip address for Zookeeper node
             */
            host?: pulumi.Input<string>;
            /**
             * TCP port which used to connect to Zookeeper node
             */
            port?: pulumi.Input<number>;
            /**
             * if a secure connection to Zookeeper is required
             */
            secure?: pulumi.Input<string>;
        }

        /**
         * define default behavior for whole ClickHouseInstallation, some behavior can be re-define on cluster, shard and replica level
         * More info: https://github.com/Altinity/clickhouse-operator/blob/master/docs/custom_resource_explained.md#specdefaults
         */
        export interface ClickHouseInstallationSpecDefaultsArgs {
            /**
             * allows change `<yandex><distributed_ddl></distributed_ddl></yandex>` settings
             * More info: https://clickhouse.tech/docs/en/operations/server-configuration-parameters/settings/#server-settings-distributed_ddl
             */
            distributedDDL?: pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationSpecDefaultsDistributedddlArgs>;
            /**
             * define should replicas be specified by FQDN in `<host></host>`.
             * In case of "no" will use short hostname and clickhouse-server will use kubernetes default suffixes for DNS lookup
             * "yes" by default
             */
            replicasUseFQDN?: pulumi.Input<string>;
            /**
             * default storage management options
             */
            storageManagement?: pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationSpecDefaultsStoragemanagementArgs>;
            /**
             * optional, configuration of the templates names which will use for generate Kubernetes resources according to one or more ClickHouse clusters described in current ClickHouseInstallation (chi) resource
             */
            templates?: pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationSpecDefaultsTemplatesArgs>;
        }

        /**
         * allows change `<yandex><distributed_ddl></distributed_ddl></yandex>` settings
         * More info: https://clickhouse.tech/docs/en/operations/server-configuration-parameters/settings/#server-settings-distributed_ddl
         */
        export interface ClickHouseInstallationSpecDefaultsDistributedddlArgs {
            /**
             * Settings from this profile will be used to execute DDL queries
             */
            profile?: pulumi.Input<string>;
        }

        /**
         * default storage management options
         */
        export interface ClickHouseInstallationSpecDefaultsStoragemanagementArgs {
            /**
             * defines `PVC` provisioner - be it StatefulSet or the Operator
             */
            provisioner?: pulumi.Input<string>;
            /**
             * defines behavior of `PVC` deletion.
             * `Delete` by default, if `Retain` specified then `PVC` will be kept when deleting StatefulSet
             */
            reclaimPolicy?: pulumi.Input<string>;
        }

        /**
         * optional, configuration of the templates names which will use for generate Kubernetes resources according to one or more ClickHouse clusters described in current ClickHouseInstallation (chi) resource
         */
        export interface ClickHouseInstallationSpecDefaultsTemplatesArgs {
            /**
             * optional, template name from chi.spec.templates.serviceTemplates, allows customization for each `Service` resource which will created by `clickhouse-operator` which cover each clickhouse cluster described in `chi.spec.configuration.clusters`
             */
            clusterServiceTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.volumeClaimTemplates, allows customization each `PVC` which will mount for clickhouse data directory in each `Pod` during render and reconcile every StatefulSet.spec resource described in `chi.spec.configuration.clusters`
             */
            dataVolumeClaimTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.hostTemplates, which will apply to configure every `clickhouse-server` instance during render ConfigMap resources which will mount into `Pod`
             */
            hostTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.volumeClaimTemplates, allows customization each `PVC` which will mount for clickhouse log directory in each `Pod` during render and reconcile every StatefulSet.spec resource described in `chi.spec.configuration.clusters`
             */
            logVolumeClaimTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.podTemplates, allows customization each `Pod` resource during render and reconcile each StatefulSet.spec resource described in `chi.spec.configuration.clusters`
             */
            podTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.serviceTemplates, allows customization for each `Service` resource which will created by `clickhouse-operator` which cover each replica inside each shard inside each clickhouse cluster described in `chi.spec.configuration.clusters`
             */
            replicaServiceTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.serviceTemplates, allows customization for one `Service` resource which will created by `clickhouse-operator` which cover all clusters in whole `chi` resource
             */
            serviceTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.serviceTemplates, allows customization for each `Service` resource which will created by `clickhouse-operator` which cover each shard inside clickhouse cluster described in `chi.spec.configuration.clusters`
             */
            shardServiceTemplate?: pulumi.Input<string>;
            /**
             * DEPRECATED! VolumeClaimTemplate is deprecated in favor of DataVolumeClaimTemplate and LogVolumeClaimTemplate
             */
            volumeClaimTemplate?: pulumi.Input<string>;
        }

        /**
         * optional, allows tuning reconciling cycle for ClickhouseInstallation from clickhouse-operator side
         */
        export interface ClickHouseInstallationSpecReconcilingArgs {
            /**
             * optional, define behavior for cleanup Kubernetes resources during reconcile cycle
             */
            cleanup?: pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationSpecReconcilingCleanupArgs>;
            /**
             * timeout in seconds when `clickhouse-operator` will wait when applied `ConfigMap` during reconcile `ClickhouseInstallation` pods will updated from cache
             * see details: https://kubernetes.io/docs/concepts/configuration/configmap/#mounted-configmaps-are-updated-automatically
             */
            configMapPropagationTimeout?: pulumi.Input<number>;
            /**
             * DEPRECATED
             */
            policy?: pulumi.Input<string>;
        }

        /**
         * optional, define behavior for cleanup Kubernetes resources during reconcile cycle
         */
        export interface ClickHouseInstallationSpecReconcilingCleanupArgs {
            /**
             * what clickhouse-operator shall do when reconciling Kubernetes resources are failed, default behavior is `Retain`
             */
            reconcileFailedObjects?: pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationSpecReconcilingCleanupReconcilefailedobjectsArgs>;
            /**
             * what clickhouse-operator shall do when found Kubernetes resources which should be managed with clickhouse-operator, but not have `ownerReference` to any currently managed `ClickHouseInstallation` resource, default behavior is `Delete`
             */
            unknownObjects?: pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationSpecReconcilingCleanupUnknownobjectsArgs>;
        }

        /**
         * what clickhouse-operator shall do when reconciling Kubernetes resources are failed, default behavior is `Retain`
         */
        export interface ClickHouseInstallationSpecReconcilingCleanupReconcilefailedobjectsArgs {
            /**
             * behavior policy for failed ConfigMap reconciling, Retain by default
             */
            configMap?: pulumi.Input<string>;
            /**
             * behavior policy for failed PVC reconciling, Retain by default
             */
            pvc?: pulumi.Input<string>;
            /**
             * behavior policy for failed Service reconciling, Retain by default
             */
            service?: pulumi.Input<string>;
            /**
             * behavior policy for failed StatefulSet reconciling, Retain by default
             */
            statefulSet?: pulumi.Input<string>;
        }

        /**
         * what clickhouse-operator shall do when found Kubernetes resources which should be managed with clickhouse-operator, but not have `ownerReference` to any currently managed `ClickHouseInstallation` resource, default behavior is `Delete`
         */
        export interface ClickHouseInstallationSpecReconcilingCleanupUnknownobjectsArgs {
            /**
             * behavior policy for unknown ConfigMap, Delete by default
             */
            configMap?: pulumi.Input<string>;
            /**
             * behavior policy for unknown PVC, Delete by default
             */
            pvc?: pulumi.Input<string>;
            /**
             * behavior policy for unknown Service, Delete by default
             */
            service?: pulumi.Input<string>;
            /**
             * behavior policy for unknown StatefulSet, Delete by default
             */
            statefulSet?: pulumi.Input<string>;
        }

        /**
         * allows define templates which will use for render Kubernetes resources like StatefulSet, ConfigMap, Service, PVC, by default, clickhouse-operator have own templates, but you can override it
         */
        export interface ClickHouseInstallationSpecTemplatesArgs {
            /**
             * hostTemplate will use during apply to generate `clickhose-server` config files
             */
            hostTemplates?: pulumi.Input<pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationSpecTemplatesHosttemplatesArgs>[]>;
            /**
             * podTemplate will use during render `Pod` inside `StatefulSet.spec` and allows define rendered `Pod.spec`, pod scheduling distribution and pod zone
             * More information: https://github.com/Altinity/clickhouse-operator/blob/master/docs/custom_resource_explained.md#spectemplatespodtemplates
             */
            podTemplates?: pulumi.Input<pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationSpecTemplatesPodtemplatesArgs>[]>;
            /**
             * allows define template for rendering `Service` which would get endpoint from Pods which scoped chi-wide, cluster-wide, shard-wide, replica-wide level
             */
            serviceTemplates?: pulumi.Input<pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationSpecTemplatesServicetemplatesArgs>[]>;
            /**
             * allows define template for rendering `PVC` kubernetes resource, which would use inside `Pod` for mount clickhouse `data`, clickhouse `logs` or something else
             */
            volumeClaimTemplates?: pulumi.Input<pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationSpecTemplatesVolumeclaimtemplatesArgs>[]>;
        }

        export interface ClickHouseInstallationSpecTemplatesHosttemplatesArgs {
            /**
             * template name, could use to link inside top-level `chi.spec.defaults.templates.hostTemplate`, cluster-level `chi.spec.configuration.clusters.templates.hostTemplate`, shard-level `chi.spec.configuration.clusters.layout.shards.temlates.hostTemplate`, replica-level `chi.spec.configuration.clusters.layout.replicas.templates.hostTemplate`
             */
            name?: pulumi.Input<string>;
            /**
             * define how will distribute numeric values of named ports in `Pod.spec.containers.ports` and clickhouse-server configs
             */
            portDistribution?: pulumi.Input<pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationSpecTemplatesHosttemplatesPortdistributionArgs>[]>;
            spec?: pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationSpecTemplatesHosttemplatesSpecArgs>;
        }

        export interface ClickHouseInstallationSpecTemplatesHosttemplatesPortdistributionArgs {
            /**
             * type of distribution, when `Unspecified` (default value) then all listen ports on clickhouse-server configuration in all Pods will have the same value, when `ClusterScopeIndex` then ports will increment to offset from base value depends on shard and replica index inside cluster with combination of `chi.spec.templates.podTemlates.spec.HostNetwork` it allows setup ClickHouse cluster inside Kubernetes and provide access via external network bypass Kubernetes internal network
             */
            type?: pulumi.Input<string>;
        }

        export interface ClickHouseInstallationSpecTemplatesHosttemplatesSpecArgs {
            /**
             * optional, allows define content of any setting file inside each `Pod` where this template will apply during generate `ConfigMap` which will mount in `/etc/clickhouse-server/config.d/` or `/etc/clickhouse-server/conf.d/` or `/etc/clickhouse-server/users.d/`
             */
            files?: pulumi.Input<{[key: string]: any}>;
            /**
             * optional, setup `http_port` inside `clickhouse-server` settings for each Pod where current template will apply
             * if specified, should have equal value with `chi.spec.templates.podTemplates.spec.containers.ports[name=http]`
             * More info: https://clickhouse.tech/docs/en/interfaces/http/
             */
            httpPort?: pulumi.Input<number>;
            httpsPort?: pulumi.Input<number>;
            /**
             * optional, open insecure ports for cluster, defaults to "yes"
             */
            insecure?: pulumi.Input<string>;
            /**
             * optional, setup `interserver_http_port` inside `clickhouse-server` settings for each Pod where current template will apply
             * if specified, should have equal value with `chi.spec.templates.podTemplates.spec.containers.ports[name=interserver]`
             * More info: https://clickhouse.tech/docs/en/operations/server-configuration-parameters/settings/#interserver-http-port
             */
            interserverHTTPPort?: pulumi.Input<number>;
            /**
             * by default, hostname will generate, but this allows define custom name for each `clickhuse-server`
             */
            name?: pulumi.Input<string>;
            /**
             * optional, open secure ports
             */
            secure?: pulumi.Input<string>;
            /**
             * optional, allows configure `clickhouse-server` settings inside <yandex>...</yandex> tag in each `Pod` where this template will apply during generate `ConfigMap` which will mount in `/etc/clickhouse-server/conf.d/`
             * More details: https://clickhouse.tech/docs/en/operations/settings/settings/
             */
            settings?: pulumi.Input<{[key: string]: any}>;
            /**
             * optional, setup `tcp_port` inside `clickhouse-server` settings for each Pod where current template will apply
             * if specified, should have equal value with `chi.spec.templates.podTemplates.spec.containers.ports[name=tcp]`
             * More info: https://clickhouse.tech/docs/en/interfaces/tcp/
             */
            tcpPort?: pulumi.Input<number>;
            /**
             * be careful, this part of CRD allows override template inside template, don't use it if you don't understand what you do
             */
            templates?: pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationSpecTemplatesHosttemplatesSpecTemplatesArgs>;
            tlsPort?: pulumi.Input<number>;
        }

        /**
         * be careful, this part of CRD allows override template inside template, don't use it if you don't understand what you do
         */
        export interface ClickHouseInstallationSpecTemplatesHosttemplatesSpecTemplatesArgs {
            /**
             * optional, template name from chi.spec.templates.serviceTemplates, allows customization for each `Service` resource which will created by `clickhouse-operator` which cover each clickhouse cluster described in `chi.spec.configuration.clusters`
             */
            clusterServiceTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.volumeClaimTemplates, allows customization each `PVC` which will mount for clickhouse data directory in each `Pod` during render and reconcile every StatefulSet.spec resource described in `chi.spec.configuration.clusters`
             */
            dataVolumeClaimTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.hostTemplates, which will apply to configure every `clickhouse-server` instance during render ConfigMap resources which will mount into `Pod`
             */
            hostTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.volumeClaimTemplates, allows customization each `PVC` which will mount for clickhouse log directory in each `Pod` during render and reconcile every StatefulSet.spec resource described in `chi.spec.configuration.clusters`
             */
            logVolumeClaimTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.podTemplates, allows customization each `Pod` resource during render and reconcile each StatefulSet.spec resource described in `chi.spec.configuration.clusters`
             */
            podTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.serviceTemplates, allows customization for each `Service` resource which will created by `clickhouse-operator` which cover each replica inside each shard inside each clickhouse cluster described in `chi.spec.configuration.clusters`
             */
            replicaServiceTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.serviceTemplates, allows customization for one `Service` resource which will created by `clickhouse-operator` which cover all clusters in whole `chi` resource
             */
            serviceTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.serviceTemplates, allows customization for each `Service` resource which will created by `clickhouse-operator` which cover each shard inside clickhouse cluster described in `chi.spec.configuration.clusters`
             */
            shardServiceTemplate?: pulumi.Input<string>;
            /**
             * DEPRECATED! VolumeClaimTemplate is deprecated in favor of DataVolumeClaimTemplate and LogVolumeClaimTemplate
             */
            volumeClaimTemplate?: pulumi.Input<string>;
        }

        export interface ClickHouseInstallationSpecTemplatesPodtemplatesArgs {
            /**
             * DEPRECATED, shortcut for `chi.spec.templates.podTemplates.spec.affinity.podAntiAffinity`
             */
            distribution?: pulumi.Input<string>;
            /**
             * allows define format for generated `Pod` name, look to https://github.com/Altinity/clickhouse-operator/blob/master/docs/custom_resource_explained.md#spectemplatesservicetemplates for details about aviailable template variables
             */
            generateName?: pulumi.Input<string>;
            /**
             * allows pass standard object's metadata from template to Pod
             * More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata
             */
            metadata?: pulumi.Input<{[key: string]: any}>;
            /**
             * template name, could use to link inside top-level `chi.spec.defaults.templates.podTemplate`, cluster-level `chi.spec.configuration.clusters.templates.podTemplate`, shard-level `chi.spec.configuration.clusters.layout.shards.temlates.podTemplate`, replica-level `chi.spec.configuration.clusters.layout.replicas.templates.podTemplate`
             */
            name?: pulumi.Input<string>;
            /**
             * define ClickHouse Pod distribution policy between Kubernetes Nodes inside Shard, Replica, Namespace, CHI, another ClickHouse cluster
             */
            podDistribution?: pulumi.Input<pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationSpecTemplatesPodtemplatesPoddistributionArgs>[]>;
            /**
             * allows define whole Pod.spec inside StaefulSet.spec, look to https://kubernetes.io/docs/concepts/workloads/pods/#pod-templates for details
             */
            spec?: pulumi.Input<{[key: string]: any}>;
            /**
             * allows define custom zone name and will separate ClickHouse `Pods` between nodes, shortcut for `chi.spec.templates.podTemplates.spec.affinity.podAntiAffinity`
             */
            zone?: pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationSpecTemplatesPodtemplatesZoneArgs>;
        }

        export interface ClickHouseInstallationSpecTemplatesPodtemplatesPoddistributionArgs {
            /**
             * define, how much ClickHouse Pods could be inside selected scope with selected distribution type
             */
            number?: pulumi.Input<number>;
            /**
             * scope for apply each podDistribution
             */
            scope?: pulumi.Input<string>;
            /**
             * use for inter-pod affinity look to `pod.spec.affinity.podAntiAffinity.preferredDuringSchedulingIgnoredDuringExecution.podAffinityTerm.topologyKey`, More info: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
             */
            topologyKey?: pulumi.Input<string>;
            /**
             * you can define multiple affinity policy types
             */
            type?: pulumi.Input<string>;
        }

        /**
         * allows define custom zone name and will separate ClickHouse `Pods` between nodes, shortcut for `chi.spec.templates.podTemplates.spec.affinity.podAntiAffinity`
         */
        export interface ClickHouseInstallationSpecTemplatesPodtemplatesZoneArgs {
            /**
             * optional, if defined, allows select kubernetes nodes by label with `name` equal `key`
             */
            key?: pulumi.Input<string>;
            /**
             * optional, if defined, allows select kubernetes nodes by label with `value` in `values`
             */
            values?: pulumi.Input<pulumi.Input<string>[]>;
        }

        export interface ClickHouseInstallationSpecTemplatesServicetemplatesArgs {
            /**
             * allows define format for generated `Service` name, look to https://github.com/Altinity/clickhouse-operator/blob/master/docs/custom_resource_explained.md#spectemplatesservicetemplates for details about aviailable template variables
             */
            generateName?: pulumi.Input<string>;
            /**
             * allows pass standard object's metadata from template to Service
             * Could be use for define specificly for Cloud Provider metadata which impact to behavior of service
             * More info: https://kubernetes.io/docs/concepts/services-networking/service/
             */
            metadata?: pulumi.Input<{[key: string]: any}>;
            /**
             * template name, could use to link inside
             * chi-level `chi.spec.defaults.templates.serviceTemplate`
             * cluster-level `chi.spec.configuration.clusters.templates.clusterServiceTemplate`
             * shard-level `chi.spec.configuration.clusters.layout.shards.temlates.shardServiceTemplate`
             * replica-level `chi.spec.configuration.clusters.layout.replicas.templates.replicaServiceTemplate` or `chi.spec.configuration.clusters.layout.shards.replicas.replicaServiceTemplate`
             */
            name?: pulumi.Input<string>;
            /**
             * describe behavior of generated Service
             * More info: https://kubernetes.io/docs/concepts/services-networking/service/
             */
            spec?: pulumi.Input<{[key: string]: any}>;
        }

        export interface ClickHouseInstallationSpecTemplatesVolumeclaimtemplatesArgs {
            /**
             * allows to pass standard object's metadata from template to PVC
             * More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata
             */
            metadata?: pulumi.Input<{[key: string]: any}>;
            /**
             * template name, could use to link inside
             * top-level `chi.spec.defaults.templates.dataVolumeClaimTemplate` or `chi.spec.defaults.templates.logVolumeClaimTemplate`,
             * cluster-level `chi.spec.configuration.clusters.templates.dataVolumeClaimTemplate` or `chi.spec.configuration.clusters.templates.logVolumeClaimTemplate`,
             * shard-level `chi.spec.configuration.clusters.layout.shards.temlates.dataVolumeClaimTemplate` or `chi.spec.configuration.clusters.layout.shards.temlates.logVolumeClaimTemplate`
             * replica-level `chi.spec.configuration.clusters.layout.replicas.templates.dataVolumeClaimTemplate` or `chi.spec.configuration.clusters.layout.replicas.templates.logVolumeClaimTemplate`
             */
            name?: pulumi.Input<string>;
            /**
             * defines `PVC` provisioner - be it StatefulSet or the Operator
             */
            provisioner?: pulumi.Input<string>;
            /**
             * defines behavior of `PVC` deletion.
             * `Delete` by default, if `Retain` specified then `PVC` will be kept when deleting StatefulSet
             */
            reclaimPolicy?: pulumi.Input<string>;
            /**
             * allows define all aspects of `PVC` resource
             * More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims
             */
            spec?: pulumi.Input<{[key: string]: any}>;
        }

        /**
         * optional, define policy for auto applying ClickHouseInstallationTemplate inside ClickHouseInstallation
         */
        export interface ClickHouseInstallationSpecTemplatingArgs {
            /**
             * when defined as `auto` inside ClickhouseInstallationTemplate, it will auto add into all ClickHouseInstallation, manual value is default
             */
            policy?: pulumi.Input<string>;
        }

        export interface ClickHouseInstallationSpecUsetemplatesArgs {
            /**
             * name of `ClickHouseInstallationTemplate` (chit) resource
             */
            name?: pulumi.Input<string>;
            /**
             * Kubernetes namespace where need search `chit` resource, depending on `watchNamespaces` settings in `clichouse-operator`
             */
            namespace?: pulumi.Input<string>;
            /**
             * optional, current strategy is only merge, and current `chi` settings have more priority than merged template `chit`
             */
            useType?: pulumi.Input<string>;
        }

        /**
         * Current ClickHouseInstallation manifest status, contains many fields like a normalized configuration, clickhouse-operator version, current action and all applied action list, current taskID and all applied taskIDs and other
         */
        export interface ClickHouseInstallationStatusArgs {
            /**
             * Action
             */
            action?: pulumi.Input<string>;
            /**
             * Actions
             */
            actions?: pulumi.Input<pulumi.Input<string>[]>;
            /**
             * ClickHouse operator git commit SHA
             */
            chopCommit?: pulumi.Input<string>;
            /**
             * ClickHouse operator build date
             */
            chopDate?: pulumi.Input<string>;
            /**
             * IP address of the operator's pod which managed this CHI
             */
            chopIp?: pulumi.Input<string>;
            /**
             * ClickHouse operator version
             */
            chopVersion?: pulumi.Input<string>;
            /**
             * Clusters count
             */
            clusters?: pulumi.Input<number>;
            /**
             * Endpoint
             */
            endpoint?: pulumi.Input<string>;
            /**
             * Last error
             */
            error?: pulumi.Input<string>;
            /**
             * Errors
             */
            errors?: pulumi.Input<pulumi.Input<string>[]>;
            /**
             * Pods FQDNs
             */
            fqdns?: pulumi.Input<pulumi.Input<string>[]>;
            /**
             * Generation
             */
            generation?: pulumi.Input<number>;
            /**
             * Hosts count
             */
            hosts?: pulumi.Input<number>;
            /**
             * Added Hosts count
             */
            hostsAdded?: pulumi.Input<number>;
            /**
             * Completed Hosts count
             */
            hostsCompleted?: pulumi.Input<number>;
            /**
             * About to delete Hosts count
             */
            hostsDelete?: pulumi.Input<number>;
            /**
             * Deleted Hosts count
             */
            hostsDeleted?: pulumi.Input<number>;
            /**
             * Updated Hosts count
             */
            hostsUpdated?: pulumi.Input<number>;
            /**
             * List of hosts with tables created by the operator
             */
            hostsWithTablesCreated?: pulumi.Input<pulumi.Input<string>[]>;
            /**
             * Normalized CHI requested
             */
            normalized?: pulumi.Input<{[key: string]: any}>;
            /**
             * Normalized CHI completed
             */
            normalizedCompleted?: pulumi.Input<{[key: string]: any}>;
            /**
             * Pod IPs
             */
            podIps?: pulumi.Input<pulumi.Input<string>[]>;
            /**
             * Pods
             */
            pods?: pulumi.Input<pulumi.Input<string>[]>;
            /**
             * Replicas count
             */
            replicas?: pulumi.Input<number>;
            /**
             * Shards count
             */
            shards?: pulumi.Input<number>;
            /**
             * Status
             */
            status?: pulumi.Input<string>;
            /**
             * Current task id
             */
            taskID?: pulumi.Input<string>;
            /**
             * Completed task ids
             */
            taskIDsCompleted?: pulumi.Input<pulumi.Input<string>[]>;
            /**
             * Started task ids
             */
            taskIDsStarted?: pulumi.Input<pulumi.Input<string>[]>;
        }

        /**
         * Specification of the desired behavior of one or more ClickHouse clusters
         * More info: https://github.com/Altinity/clickhouse-operator/blob/master/docs/custom_resource_explained.md
         */
        export interface ClickHouseInstallationTemplateSpecArgs {
            /**
             * allows configure multiple aspects and behavior for `clickhouse-server` instance and also allows describe multiple `clickhouse-server` clusters inside one `chi` resource
             */
            configuration?: pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationTemplateSpecConfigurationArgs>;
            /**
             * define default behavior for whole ClickHouseInstallation, some behavior can be re-define on cluster, shard and replica level
             * More info: https://github.com/Altinity/clickhouse-operator/blob/master/docs/custom_resource_explained.md#specdefaults
             */
            defaults?: pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationTemplateSpecDefaultsArgs>;
            /**
             * custom domain suffix which will add to end of `Service` or `Pod` name, use it when you use custom cluster domain in your Kubernetes cluster
             */
            namespaceDomainPattern?: pulumi.Input<string>;
            /**
             * optional, allows tuning reconciling cycle for ClickhouseInstallation from clickhouse-operator side
             */
            reconciling?: pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationTemplateSpecReconcilingArgs>;
            /**
             * This is a 'soft restart' button. When set to 'RollingUpdate' operator will restart ClickHouse pods in a graceful way. Remove it after the use in order to avoid unneeded restarts
             */
            restart?: pulumi.Input<string>;
            /**
             * Allow stop all ClickHouse clusters described in current chi.
             * Stop mechanism works as follows:
             *  - When `stop` is `1` then setup `Replicas: 0` in each related to current `chi` StatefulSet resource, all `Pods` and `Service` resources will desctroy, but PVCs still live
             *  - When `stop` is `0` then `Pods` will created again and will attach retained PVCs and `Service` also will created again
             */
            stop?: pulumi.Input<string>;
            /**
             * Allows to define custom taskID for named update operation and watch status of this update execution in .status.taskIDs field.
             * By default every update of chi manifest will generate random taskID
             */
            taskID?: pulumi.Input<string>;
            /**
             * allows define templates which will use for render Kubernetes resources like StatefulSet, ConfigMap, Service, PVC, by default, clickhouse-operator have own templates, but you can override it
             */
            templates?: pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationTemplateSpecTemplatesArgs>;
            /**
             * optional, define policy for auto applying ClickHouseInstallationTemplate inside ClickHouseInstallation
             */
            templating?: pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationTemplateSpecTemplatingArgs>;
            /**
             * allows troubleshoot Pods during CrashLoopBack state, when you apply wrong configuration, `clickhouse-server` wouldn't startup
             */
            troubleshoot?: pulumi.Input<string>;
            /**
             * list of `ClickHouseInstallationTemplate` (chit) resource names which will merge with current `Chi` manifest during render Kubernetes resources to create related ClickHouse clusters
             */
            useTemplates?: pulumi.Input<pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationTemplateSpecUsetemplatesArgs>[]>;
        }

        /**
         * allows configure multiple aspects and behavior for `clickhouse-server` instance and also allows describe multiple `clickhouse-server` clusters inside one `chi` resource
         */
        export interface ClickHouseInstallationTemplateSpecConfigurationArgs {
            /**
             * describes ClickHouse clusters layout and allows change settings on cluster-level, shard-level and replica-level
             * every cluster is a set of StatefulSet, one StatefulSet contains only one Pod with `clickhouse-server`
             * all Pods will rendered in <remote_server> part of ClickHouse configs, mounted from ConfigMap as `/etc/clickhouse-server/config.d/chop-generated-remote_servers.xml`
             * Clusters will use for Distributed table engine, more details: https://clickhouse.tech/docs/en/engines/table-engines/special/distributed/
             * If `cluster` contains zookeeper settings (could be inherited from top `chi` level), when you can create *ReplicatedMergeTree tables
             */
            clusters?: pulumi.Input<pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationTemplateSpecConfigurationClustersArgs>[]>;
            /**
             * allows define content of any setting file inside each `Pod` during generate `ConfigMap` which will mount in `/etc/clickhouse-server/config.d/` or `/etc/clickhouse-server/conf.d/` or `/etc/clickhouse-server/users.d/`
             * every key in this object is the file name
             * every value in this object is the file content
             * you can use `!!binary |` and base64 for binary files, see details here https://yaml.org/type/binary.html
             * each key could contains prefix like USERS, COMMON, HOST or config.d, users.d, cond.d, wrong prefixes will ignored, subfolders also will ignored
             * More details: https://github.com/Altinity/clickhouse-operator/blob/master/docs/chi-examples/05-settings-05-files-nested.yaml
             */
            files?: pulumi.Input<{[key: string]: any}>;
            /**
             * allows configure <yandex><profiles>..</profiles></yandex> section in each `Pod` during generate `ConfigMap` which will mount in `/etc/clickhouse-server/users.d/`
             * you can configure any aspect of settings profile
             * More details: https://clickhouse.tech/docs/en/operations/settings/settings-profiles/
             * Your yaml code will convert to XML, see examples https://github.com/Altinity/clickhouse-operator/blob/master/docs/custom_resource_explained.md#specconfigurationprofiles
             */
            profiles?: pulumi.Input<{[key: string]: any}>;
            /**
             * allows configure <yandex><quotas>..</quotas></yandex> section in each `Pod` during generate `ConfigMap` which will mount in `/etc/clickhouse-server/users.d/`
             * you can configure any aspect of resource quotas
             * More details: https://clickhouse.tech/docs/en/operations/quotas/
             * Your yaml code will convert to XML, see examples https://github.com/Altinity/clickhouse-operator/blob/master/docs/custom_resource_explained.md#specconfigurationquotas
             */
            quotas?: pulumi.Input<{[key: string]: any}>;
            /**
             * allows configure `clickhouse-server` settings inside <yandex>...</yandex> tag in each `Pod` during generate `ConfigMap` which will mount in `/etc/clickhouse-server/config.d/`
             * More details: https://clickhouse.tech/docs/en/operations/settings/settings/
             * Your yaml code will convert to XML, see examples https://github.com/Altinity/clickhouse-operator/blob/master/docs/custom_resource_explained.md#specconfigurationsettings
             */
            settings?: pulumi.Input<{[key: string]: any}>;
            /**
             * allows configure <yandex><users>..</users></yandex> section in each `Pod` during generate `ConfigMap` which will mount in `/etc/clickhouse-server/users.d/`
             * you can configure password hashed, authorization restrictions, database level security row filters etc.
             * More details: https://clickhouse.tech/docs/en/operations/settings/settings-users/
             * Your yaml code will convert to XML, see examples https://github.com/Altinity/clickhouse-operator/blob/master/docs/custom_resource_explained.md#specconfigurationusers
             */
            users?: pulumi.Input<{[key: string]: any}>;
            /**
             * allows configure <yandex><zookeeper>..</zookeeper></yandex> section in each `Pod` during generate `ConfigMap` which will mounted in `/etc/clickhouse-server/config.d/`
             * `clickhouse-operator` itself doesn't manage Zookeeper, please install Zookeeper separatelly look examples on https://github.com/Altinity/clickhouse-operator/tree/master/deploy/zookeeper/
             * currently, zookeeper (or clickhouse-keeper replacement) used for *ReplicatedMergeTree table engines and for `distributed_ddl`
             * More details: https://clickhouse.tech/docs/en/operations/server-configuration-parameters/settings/#server-settings_zookeeper
             */
            zookeeper?: pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationTemplateSpecConfigurationZookeeperArgs>;
        }

        export interface ClickHouseInstallationTemplateSpecConfigurationClustersArgs {
            /**
             * optional, allows define content of any setting file inside each `Pod` on current cluster during generate `ConfigMap` which will mount in `/etc/clickhouse-server/config.d/` or `/etc/clickhouse-server/conf.d/` or `/etc/clickhouse-server/users.d/`
             * override top-level `chi.spec.configuration.files`
             */
            files?: pulumi.Input<{[key: string]: any}>;
            /**
             * optional, open insecure ports for cluster, defaults to "yes"
             */
            insecure?: pulumi.Input<string>;
            /**
             * describe current cluster layout, how much shards in cluster, how much replica in shard
             * allows override settings on each shard and replica separatelly
             */
            layout?: pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationTemplateSpecConfigurationClustersLayoutArgs>;
            /**
             * cluster name, used to identify set of ClickHouse servers and wide used during generate names of related Kubernetes resources
             */
            name?: pulumi.Input<string>;
            /**
             * describes how schema is propagated within replicas and shards
             */
            schemaPolicy?: pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationTemplateSpecConfigurationClustersSchemapolicyArgs>;
            /**
             * optional, shared secret value to secure cluster communications
             */
            secret?: pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationTemplateSpecConfigurationClustersSecretArgs>;
            /**
             * optional, open secure ports for cluster
             */
            secure?: pulumi.Input<string>;
            /**
             * optional, allows configure `clickhouse-server` settings inside <yandex>...</yandex> tag in each `Pod` only in one cluster during generate `ConfigMap` which will mount in `/etc/clickhouse-server/config.d/`
             * override top-level `chi.spec.configuration.settings`
             * More details: https://clickhouse.tech/docs/en/operations/settings/settings/
             */
            settings?: pulumi.Input<{[key: string]: any}>;
            /**
             * optional, configuration of the templates names which will use for generate Kubernetes resources according to selected cluster
             * override top-level `chi.spec.configuration.templates`
             */
            templates?: pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationTemplateSpecConfigurationClustersTemplatesArgs>;
            /**
             * optional, allows configure <yandex><zookeeper>..</zookeeper></yandex> section in each `Pod` only in current ClickHouse cluster, during generate `ConfigMap` which will mounted in `/etc/clickhouse-server/config.d/`
             * override top-level `chi.spec.configuration.zookeeper` settings
             */
            zookeeper?: pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationTemplateSpecConfigurationClustersZookeeperArgs>;
        }

        /**
         * describe current cluster layout, how much shards in cluster, how much replica in shard
         * allows override settings on each shard and replica separatelly
         */
        export interface ClickHouseInstallationTemplateSpecConfigurationClustersLayoutArgs {
            /**
             * optional, allows override top-level `chi.spec.configuration` and cluster-level `chi.spec.configuration.clusters` configuration for each replica and each shard relates to selected replica, use it only if you fully understand what you do
             */
            replicas?: pulumi.Input<pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationTemplateSpecConfigurationClustersLayoutReplicasArgs>[]>;
            /**
             * how much replicas in each shards for current ClickHouse cluster will run in Kubernetes, each replica is a separate `StatefulSet` which contains only one `Pod` with `clickhouse-server` instance, every shard contains 1 replica by default
             */
            replicasCount?: pulumi.Input<number>;
            /**
             * optional, allows override top-level `chi.spec.configuration`, cluster-level `chi.spec.configuration.clusters` settings for each shard separately, use it only if you fully understand what you do
             */
            shards?: pulumi.Input<pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationTemplateSpecConfigurationClustersLayoutShardsArgs>[]>;
            /**
             * how much shards for current ClickHouse cluster will run in Kubernetes, each shard contains shared-nothing part of data and contains set of replicas, cluster contains 1 shard by default
             */
            shardsCount?: pulumi.Input<number>;
            /**
             * DEPRECATED - to be removed soon
             */
            type?: pulumi.Input<string>;
        }

        export interface ClickHouseInstallationTemplateSpecConfigurationClustersLayoutReplicasArgs {
            /**
             * optional, allows define content of any setting file inside each `Pod` only in one replica during generate `ConfigMap` which will mount in `/etc/clickhouse-server/config.d/` or `/etc/clickhouse-server/conf.d/` or `/etc/clickhouse-server/users.d/`
             * override top-level `chi.spec.configuration.files` and cluster-level `chi.spec.configuration.clusters.files`, will ignore if `chi.spec.configuration.clusters.layout.shards` presents
             */
            files?: pulumi.Input<{[key: string]: any}>;
            /**
             * optional, by default replica name is generated, but you can override it and setup custom name
             */
            name?: pulumi.Input<string>;
            /**
             * optional, allows configure `clickhouse-server` settings inside <yandex>...</yandex> tag in `Pod` only in one replica during generate `ConfigMap` which will mount in `/etc/clickhouse-server/conf.d/`
             * override top-level `chi.spec.configuration.settings`, cluster-level `chi.spec.configuration.clusters.settings` and will ignore if shard-level `chi.spec.configuration.clusters.layout.shards` present
             * More details: https://clickhouse.tech/docs/en/operations/settings/settings/
             */
            settings?: pulumi.Input<{[key: string]: any}>;
            /**
             * optional, list of shards related to current replica, will ignore if `chi.spec.configuration.clusters.layout.shards` presents
             */
            shards?: pulumi.Input<pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationTemplateSpecConfigurationClustersLayoutReplicasShardsArgs>[]>;
            /**
             * optional, count of shards related to current replica, you can override each shard behavior on low-level `chi.spec.configuration.clusters.layout.replicas.shards`
             */
            shardsCount?: pulumi.Input<number>;
            /**
             * optional, configuration of the templates names which will use for generate Kubernetes resources according to selected replica
             * override top-level `chi.spec.configuration.templates`, cluster-level `chi.spec.configuration.clusters.templates`
             */
            templates?: pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationTemplateSpecConfigurationClustersLayoutReplicasTemplatesArgs>;
        }

        export interface ClickHouseInstallationTemplateSpecConfigurationClustersLayoutReplicasShardsArgs {
            /**
             * optional, allows define content of any setting file inside each `Pod` only in one shard related to current replica during generate `ConfigMap` which will mount in `/etc/clickhouse-server/config.d/` or `/etc/clickhouse-server/conf.d/` or `/etc/clickhouse-server/users.d/`
             * override top-level `chi.spec.configuration.files` and cluster-level `chi.spec.configuration.clusters.files`, will ignore if `chi.spec.configuration.clusters.layout.shards` presents
             */
            files?: pulumi.Input<{[key: string]: any}>;
            /**
             * optional, setup `Pod.spec.containers.ports` with name `http` for selected shard, override `chi.spec.templates.hostTemplates.spec.httpPort`
             * allows connect to `clickhouse-server` via HTTP protocol via kubernetes `Service`
             */
            httpPort?: pulumi.Input<number>;
            httpsPort?: pulumi.Input<number>;
            /**
             * optional, open insecure ports for cluster, defaults to "yes"
             */
            insecure?: pulumi.Input<string>;
            /**
             * optional, setup `Pod.spec.containers.ports` with name `interserver` for selected shard, override `chi.spec.templates.hostTemplates.spec.interserverHTTPPort`
             * allows connect between replicas inside same shard during fetch replicated data parts HTTP protocol
             */
            interserverHTTPPort?: pulumi.Input<number>;
            /**
             * optional, by default shard name is generated, but you can override it and setup custom name
             */
            name?: pulumi.Input<string>;
            /**
             * optional, open secure ports
             */
            secure?: pulumi.Input<string>;
            /**
             * optional, allows configure `clickhouse-server` settings inside <yandex>...</yandex> tag in `Pod` only in one shard related to current replica during generate `ConfigMap` which will mount in `/etc/clickhouse-server/conf.d/`
             * override top-level `chi.spec.configuration.settings`, cluster-level `chi.spec.configuration.clusters.settings` and replica-level `chi.spec.configuration.clusters.layout.replicas.settings`
             * More details: https://clickhouse.tech/docs/en/operations/settings/settings/
             */
            settings?: pulumi.Input<{[key: string]: any}>;
            /**
             * optional, setup `Pod.spec.containers.ports` with name `tcp` for selected shard, override `chi.spec.templates.hostTemplates.spec.tcpPort`
             * allows connect to `clickhouse-server` via TCP Native protocol via kubernetes `Service`
             */
            tcpPort?: pulumi.Input<number>;
            /**
             * optional, configuration of the templates names which will use for generate Kubernetes resources according to selected replica
             * override top-level `chi.spec.configuration.templates`, cluster-level `chi.spec.configuration.clusters.templates`, replica-level `chi.spec.configuration.clusters.layout.replicas.templates`
             */
            templates?: pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationTemplateSpecConfigurationClustersLayoutReplicasShardsTemplatesArgs>;
            tlsPort?: pulumi.Input<number>;
        }

        /**
         * optional, configuration of the templates names which will use for generate Kubernetes resources according to selected replica
         * override top-level `chi.spec.configuration.templates`, cluster-level `chi.spec.configuration.clusters.templates`, replica-level `chi.spec.configuration.clusters.layout.replicas.templates`
         */
        export interface ClickHouseInstallationTemplateSpecConfigurationClustersLayoutReplicasShardsTemplatesArgs {
            /**
             * optional, template name from chi.spec.templates.serviceTemplates, allows customization for each `Service` resource which will created by `clickhouse-operator` which cover each clickhouse cluster described in `chi.spec.configuration.clusters`
             */
            clusterServiceTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.volumeClaimTemplates, allows customization each `PVC` which will mount for clickhouse data directory in each `Pod` during render and reconcile every StatefulSet.spec resource described in `chi.spec.configuration.clusters`
             */
            dataVolumeClaimTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.hostTemplates, which will apply to configure every `clickhouse-server` instance during render ConfigMap resources which will mount into `Pod`
             */
            hostTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.volumeClaimTemplates, allows customization each `PVC` which will mount for clickhouse log directory in each `Pod` during render and reconcile every StatefulSet.spec resource described in `chi.spec.configuration.clusters`
             */
            logVolumeClaimTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.podTemplates, allows customization each `Pod` resource during render and reconcile each StatefulSet.spec resource described in `chi.spec.configuration.clusters`
             */
            podTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.serviceTemplates, allows customization for each `Service` resource which will created by `clickhouse-operator` which cover each replica inside each shard inside each clickhouse cluster described in `chi.spec.configuration.clusters`
             */
            replicaServiceTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.serviceTemplates, allows customization for one `Service` resource which will created by `clickhouse-operator` which cover all clusters in whole `chi` resource
             */
            serviceTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.serviceTemplates, allows customization for each `Service` resource which will created by `clickhouse-operator` which cover each shard inside clickhouse cluster described in `chi.spec.configuration.clusters`
             */
            shardServiceTemplate?: pulumi.Input<string>;
            /**
             * DEPRECATED! VolumeClaimTemplate is deprecated in favor of DataVolumeClaimTemplate and LogVolumeClaimTemplate
             */
            volumeClaimTemplate?: pulumi.Input<string>;
        }

        /**
         * optional, configuration of the templates names which will use for generate Kubernetes resources according to selected replica
         * override top-level `chi.spec.configuration.templates`, cluster-level `chi.spec.configuration.clusters.templates`
         */
        export interface ClickHouseInstallationTemplateSpecConfigurationClustersLayoutReplicasTemplatesArgs {
            /**
             * optional, template name from chi.spec.templates.serviceTemplates, allows customization for each `Service` resource which will created by `clickhouse-operator` which cover each clickhouse cluster described in `chi.spec.configuration.clusters`
             */
            clusterServiceTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.volumeClaimTemplates, allows customization each `PVC` which will mount for clickhouse data directory in each `Pod` during render and reconcile every StatefulSet.spec resource described in `chi.spec.configuration.clusters`
             */
            dataVolumeClaimTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.hostTemplates, which will apply to configure every `clickhouse-server` instance during render ConfigMap resources which will mount into `Pod`
             */
            hostTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.volumeClaimTemplates, allows customization each `PVC` which will mount for clickhouse log directory in each `Pod` during render and reconcile every StatefulSet.spec resource described in `chi.spec.configuration.clusters`
             */
            logVolumeClaimTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.podTemplates, allows customization each `Pod` resource during render and reconcile each StatefulSet.spec resource described in `chi.spec.configuration.clusters`
             */
            podTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.serviceTemplates, allows customization for each `Service` resource which will created by `clickhouse-operator` which cover each replica inside each shard inside each clickhouse cluster described in `chi.spec.configuration.clusters`
             */
            replicaServiceTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.serviceTemplates, allows customization for one `Service` resource which will created by `clickhouse-operator` which cover all clusters in whole `chi` resource
             */
            serviceTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.serviceTemplates, allows customization for each `Service` resource which will created by `clickhouse-operator` which cover each shard inside clickhouse cluster described in `chi.spec.configuration.clusters`
             */
            shardServiceTemplate?: pulumi.Input<string>;
            /**
             * DEPRECATED! VolumeClaimTemplate is deprecated in favor of DataVolumeClaimTemplate and LogVolumeClaimTemplate
             */
            volumeClaimTemplate?: pulumi.Input<string>;
        }

        export interface ClickHouseInstallationTemplateSpecConfigurationClustersLayoutShardsArgs {
            /**
             * DEPRECATED - to be removed soon
             */
            definitionType?: pulumi.Input<string>;
            /**
             * optional, allows define content of any setting file inside each `Pod` only in one shard during generate `ConfigMap` which will mount in `/etc/clickhouse-server/config.d/` or `/etc/clickhouse-server/conf.d/` or `/etc/clickhouse-server/users.d/`
             * override top-level `chi.spec.configuration.files` and cluster-level `chi.spec.configuration.clusters.files`
             */
            files?: pulumi.Input<{[key: string]: any}>;
            /**
             * optional, `true` by default when `chi.spec.configuration.clusters[].layout.ReplicaCount` > 1 and 0 otherwise
             * allows setup <internal_replication> setting which will use during insert into tables with `Distributed` engine for insert only in one live replica and other replicas will download inserted data during replication,
             * will apply in <remote_servers> inside ConfigMap which will mount in /etc/clickhouse-server/config.d/chop-generated-remote_servers.xml
             * More details: https://clickhouse.tech/docs/en/engines/table-engines/special/distributed/
             */
            internalReplication?: pulumi.Input<string>;
            /**
             * optional, by default shard name is generated, but you can override it and setup custom name
             */
            name?: pulumi.Input<string>;
            /**
             * optional, allows override behavior for selected replicas from cluster-level `chi.spec.configuration.clusters` and shard-level `chi.spec.configuration.clusters.layout.shards`
             */
            replicas?: pulumi.Input<pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationTemplateSpecConfigurationClustersLayoutShardsReplicasArgs>[]>;
            /**
             * optional, how much replicas in selected shard for selected ClickHouse cluster will run in Kubernetes, each replica is a separate `StatefulSet` which contains only one `Pod` with `clickhouse-server` instance,
             * shard contains 1 replica by default
             * override cluster-level `chi.spec.configuration.clusters.layout.replicasCount`
             */
            replicasCount?: pulumi.Input<number>;
            /**
             * optional, allows configure `clickhouse-server` settings inside <yandex>...</yandex> tag in each `Pod` only in one shard during generate `ConfigMap` which will mount in `/etc/clickhouse-server/config.d/`
             * override top-level `chi.spec.configuration.settings` and cluster-level `chi.spec.configuration.clusters.settings`
             * More details: https://clickhouse.tech/docs/en/operations/settings/settings/
             */
            settings?: pulumi.Input<{[key: string]: any}>;
            /**
             * optional, configuration of the templates names which will use for generate Kubernetes resources according to selected shard
             * override top-level `chi.spec.configuration.templates` and cluster-level `chi.spec.configuration.clusters.templates`
             */
            templates?: pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationTemplateSpecConfigurationClustersLayoutShardsTemplatesArgs>;
            /**
             * optional, 1 by default, allows setup shard <weight> setting which will use during insert into tables with `Distributed` engine,
             * will apply in <remote_servers> inside ConfigMap which will mount in /etc/clickhouse-server/config.d/chop-generated-remote_servers.xml
             * More details: https://clickhouse.tech/docs/en/engines/table-engines/special/distributed/
             */
            weight?: pulumi.Input<number>;
        }

        export interface ClickHouseInstallationTemplateSpecConfigurationClustersLayoutShardsReplicasArgs {
            /**
             * optional, allows define content of any setting file inside `Pod` only in one replica during generate `ConfigMap` which will mount in `/etc/clickhouse-server/config.d/` or `/etc/clickhouse-server/conf.d/` or `/etc/clickhouse-server/users.d/`
             * override top-level `chi.spec.configuration.files`, cluster-level `chi.spec.configuration.clusters.files` and shard-level `chi.spec.configuration.clusters.layout.shards.files`
             */
            files?: pulumi.Input<{[key: string]: any}>;
            /**
             * optional, setup `Pod.spec.containers.ports` with name `http` for selected replica, override `chi.spec.templates.hostTemplates.spec.httpPort`
             * allows connect to `clickhouse-server` via HTTP protocol via kubernetes `Service`
             */
            httpPort?: pulumi.Input<number>;
            httpsPort?: pulumi.Input<number>;
            /**
             * optional, open insecure ports for cluster, defaults to "yes"
             */
            insecure?: pulumi.Input<string>;
            /**
             * optional, setup `Pod.spec.containers.ports` with name `interserver` for selected replica, override `chi.spec.templates.hostTemplates.spec.interserverHTTPPort`
             * allows connect between replicas inside same shard during fetch replicated data parts HTTP protocol
             */
            interserverHTTPPort?: pulumi.Input<number>;
            /**
             * optional, by default replica name is generated, but you can override it and setup custom name
             */
            name?: pulumi.Input<string>;
            /**
             * optional, open secure ports
             */
            secure?: pulumi.Input<string>;
            /**
             * optional, allows configure `clickhouse-server` settings inside <yandex>...</yandex> tag in `Pod` only in one replica during generate `ConfigMap` which will mount in `/etc/clickhouse-server/conf.d/`
             * override top-level `chi.spec.configuration.settings`, cluster-level `chi.spec.configuration.clusters.settings` and shard-level `chi.spec.configuration.clusters.layout.shards.settings`
             * More details: https://clickhouse.tech/docs/en/operations/settings/settings/
             */
            settings?: pulumi.Input<{[key: string]: any}>;
            /**
             * optional, setup `Pod.spec.containers.ports` with name `tcp` for selected replica, override `chi.spec.templates.hostTemplates.spec.tcpPort`
             * allows connect to `clickhouse-server` via TCP Native protocol via kubernetes `Service`
             */
            tcpPort?: pulumi.Input<number>;
            /**
             * optional, configuration of the templates names which will use for generate Kubernetes resources according to selected replica
             * override top-level `chi.spec.configuration.templates`, cluster-level `chi.spec.configuration.clusters.templates` and shard-level `chi.spec.configuration.clusters.layout.shards.templates`
             */
            templates?: pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationTemplateSpecConfigurationClustersLayoutShardsReplicasTemplatesArgs>;
            tlsPort?: pulumi.Input<number>;
        }

        /**
         * optional, configuration of the templates names which will use for generate Kubernetes resources according to selected replica
         * override top-level `chi.spec.configuration.templates`, cluster-level `chi.spec.configuration.clusters.templates` and shard-level `chi.spec.configuration.clusters.layout.shards.templates`
         */
        export interface ClickHouseInstallationTemplateSpecConfigurationClustersLayoutShardsReplicasTemplatesArgs {
            /**
             * optional, template name from chi.spec.templates.serviceTemplates, allows customization for each `Service` resource which will created by `clickhouse-operator` which cover each clickhouse cluster described in `chi.spec.configuration.clusters`
             */
            clusterServiceTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.volumeClaimTemplates, allows customization each `PVC` which will mount for clickhouse data directory in each `Pod` during render and reconcile every StatefulSet.spec resource described in `chi.spec.configuration.clusters`
             */
            dataVolumeClaimTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.hostTemplates, which will apply to configure every `clickhouse-server` instance during render ConfigMap resources which will mount into `Pod`
             */
            hostTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.volumeClaimTemplates, allows customization each `PVC` which will mount for clickhouse log directory in each `Pod` during render and reconcile every StatefulSet.spec resource described in `chi.spec.configuration.clusters`
             */
            logVolumeClaimTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.podTemplates, allows customization each `Pod` resource during render and reconcile each StatefulSet.spec resource described in `chi.spec.configuration.clusters`
             */
            podTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.serviceTemplates, allows customization for each `Service` resource which will created by `clickhouse-operator` which cover each replica inside each shard inside each clickhouse cluster described in `chi.spec.configuration.clusters`
             */
            replicaServiceTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.serviceTemplates, allows customization for one `Service` resource which will created by `clickhouse-operator` which cover all clusters in whole `chi` resource
             */
            serviceTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.serviceTemplates, allows customization for each `Service` resource which will created by `clickhouse-operator` which cover each shard inside clickhouse cluster described in `chi.spec.configuration.clusters`
             */
            shardServiceTemplate?: pulumi.Input<string>;
            /**
             * DEPRECATED! VolumeClaimTemplate is deprecated in favor of DataVolumeClaimTemplate and LogVolumeClaimTemplate
             */
            volumeClaimTemplate?: pulumi.Input<string>;
        }

        /**
         * optional, configuration of the templates names which will use for generate Kubernetes resources according to selected shard
         * override top-level `chi.spec.configuration.templates` and cluster-level `chi.spec.configuration.clusters.templates`
         */
        export interface ClickHouseInstallationTemplateSpecConfigurationClustersLayoutShardsTemplatesArgs {
            /**
             * optional, template name from chi.spec.templates.serviceTemplates, allows customization for each `Service` resource which will created by `clickhouse-operator` which cover each clickhouse cluster described in `chi.spec.configuration.clusters`
             */
            clusterServiceTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.volumeClaimTemplates, allows customization each `PVC` which will mount for clickhouse data directory in each `Pod` during render and reconcile every StatefulSet.spec resource described in `chi.spec.configuration.clusters`
             */
            dataVolumeClaimTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.hostTemplates, which will apply to configure every `clickhouse-server` instance during render ConfigMap resources which will mount into `Pod`
             */
            hostTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.volumeClaimTemplates, allows customization each `PVC` which will mount for clickhouse log directory in each `Pod` during render and reconcile every StatefulSet.spec resource described in `chi.spec.configuration.clusters`
             */
            logVolumeClaimTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.podTemplates, allows customization each `Pod` resource during render and reconcile each StatefulSet.spec resource described in `chi.spec.configuration.clusters`
             */
            podTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.serviceTemplates, allows customization for each `Service` resource which will created by `clickhouse-operator` which cover each replica inside each shard inside each clickhouse cluster described in `chi.spec.configuration.clusters`
             */
            replicaServiceTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.serviceTemplates, allows customization for one `Service` resource which will created by `clickhouse-operator` which cover all clusters in whole `chi` resource
             */
            serviceTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.serviceTemplates, allows customization for each `Service` resource which will created by `clickhouse-operator` which cover each shard inside clickhouse cluster described in `chi.spec.configuration.clusters`
             */
            shardServiceTemplate?: pulumi.Input<string>;
            /**
             * DEPRECATED! VolumeClaimTemplate is deprecated in favor of DataVolumeClaimTemplate and LogVolumeClaimTemplate
             */
            volumeClaimTemplate?: pulumi.Input<string>;
        }

        /**
         * describes how schema is propagated within replicas and shards
         */
        export interface ClickHouseInstallationTemplateSpecConfigurationClustersSchemapolicyArgs {
            /**
             * how schema is propagated within a replica
             */
            replica?: pulumi.Input<string>;
            /**
             * how schema is propagated between shards
             */
            shard?: pulumi.Input<string>;
        }

        /**
         * optional, shared secret value to secure cluster communications
         */
        export interface ClickHouseInstallationTemplateSpecConfigurationClustersSecretArgs {
            /**
             * Auto-generate shared secret value to secure cluster communications
             */
            auto?: pulumi.Input<string>;
            /**
             * Cluster shared secret value in plain text
             */
            value?: pulumi.Input<string>;
            /**
             * Cluster shared secret source
             */
            valueFrom?: pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationTemplateSpecConfigurationClustersSecretValuefromArgs>;
        }

        /**
         * Cluster shared secret source
         */
        export interface ClickHouseInstallationTemplateSpecConfigurationClustersSecretValuefromArgs {
            /**
             * Selects a key of a secret in the clickhouse installation namespace.
             * Should not be used if value is not empty.
             */
            secretKeyRef?: pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationTemplateSpecConfigurationClustersSecretValuefromSecretkeyrefArgs>;
        }

        /**
         * Selects a key of a secret in the clickhouse installation namespace.
         * Should not be used if value is not empty.
         */
        export interface ClickHouseInstallationTemplateSpecConfigurationClustersSecretValuefromSecretkeyrefArgs {
            /**
             * The key of the secret to select from. Must be a valid secret key.
             */
            key: pulumi.Input<string>;
            /**
             * Name of the referent. More info:
             * https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names
             */
            name: pulumi.Input<string>;
            /**
             * Specify whether the Secret or its key must be defined
             */
            optional?: pulumi.Input<boolean>;
        }

        /**
         * optional, configuration of the templates names which will use for generate Kubernetes resources according to selected cluster
         * override top-level `chi.spec.configuration.templates`
         */
        export interface ClickHouseInstallationTemplateSpecConfigurationClustersTemplatesArgs {
            /**
             * optional, template name from chi.spec.templates.serviceTemplates, allows customization for each `Service` resource which will created by `clickhouse-operator` which cover each clickhouse cluster described in `chi.spec.configuration.clusters`
             */
            clusterServiceTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.volumeClaimTemplates, allows customization each `PVC` which will mount for clickhouse data directory in each `Pod` during render and reconcile every StatefulSet.spec resource described in `chi.spec.configuration.clusters`
             */
            dataVolumeClaimTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.hostTemplates, which will apply to configure every `clickhouse-server` instance during render ConfigMap resources which will mount into `Pod`
             */
            hostTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.volumeClaimTemplates, allows customization each `PVC` which will mount for clickhouse log directory in each `Pod` during render and reconcile every StatefulSet.spec resource described in `chi.spec.configuration.clusters`
             */
            logVolumeClaimTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.podTemplates, allows customization each `Pod` resource during render and reconcile each StatefulSet.spec resource described in `chi.spec.configuration.clusters`
             */
            podTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.serviceTemplates, allows customization for each `Service` resource which will created by `clickhouse-operator` which cover each replica inside each shard inside each clickhouse cluster described in `chi.spec.configuration.clusters`
             */
            replicaServiceTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.serviceTemplates, allows customization for one `Service` resource which will created by `clickhouse-operator` which cover all clusters in whole `chi` resource
             */
            serviceTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.serviceTemplates, allows customization for each `Service` resource which will created by `clickhouse-operator` which cover each shard inside clickhouse cluster described in `chi.spec.configuration.clusters`
             */
            shardServiceTemplate?: pulumi.Input<string>;
            /**
             * DEPRECATED! VolumeClaimTemplate is deprecated in favor of DataVolumeClaimTemplate and LogVolumeClaimTemplate
             */
            volumeClaimTemplate?: pulumi.Input<string>;
        }

        /**
         * optional, allows configure <yandex><zookeeper>..</zookeeper></yandex> section in each `Pod` only in current ClickHouse cluster, during generate `ConfigMap` which will mounted in `/etc/clickhouse-server/config.d/`
         * override top-level `chi.spec.configuration.zookeeper` settings
         */
        export interface ClickHouseInstallationTemplateSpecConfigurationClustersZookeeperArgs {
            /**
             * optional access credentials string with `user:password` format used when use digest authorization in Zookeeper
             */
            identity?: pulumi.Input<string>;
            /**
             * describe every available zookeeper cluster node for interaction
             */
            nodes?: pulumi.Input<pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationTemplateSpecConfigurationClustersZookeeperNodesArgs>[]>;
            /**
             * one operation timeout during Zookeeper transactions
             */
            operation_timeout_ms?: pulumi.Input<number>;
            /**
             * optional root znode path inside zookeeper to store ClickHouse related data (replication queue or distributed DDL)
             */
            root?: pulumi.Input<string>;
            /**
             * session timeout during connect to Zookeeper
             */
            session_timeout_ms?: pulumi.Input<number>;
        }

        export interface ClickHouseInstallationTemplateSpecConfigurationClustersZookeeperNodesArgs {
            /**
             * dns name or ip address for Zookeeper node
             */
            host?: pulumi.Input<string>;
            /**
             * TCP port which used to connect to Zookeeper node
             */
            port?: pulumi.Input<number>;
            /**
             * if a secure connection to Zookeeper is required
             */
            secure?: pulumi.Input<string>;
        }

        /**
         * allows configure <yandex><zookeeper>..</zookeeper></yandex> section in each `Pod` during generate `ConfigMap` which will mounted in `/etc/clickhouse-server/config.d/`
         * `clickhouse-operator` itself doesn't manage Zookeeper, please install Zookeeper separatelly look examples on https://github.com/Altinity/clickhouse-operator/tree/master/deploy/zookeeper/
         * currently, zookeeper (or clickhouse-keeper replacement) used for *ReplicatedMergeTree table engines and for `distributed_ddl`
         * More details: https://clickhouse.tech/docs/en/operations/server-configuration-parameters/settings/#server-settings_zookeeper
         */
        export interface ClickHouseInstallationTemplateSpecConfigurationZookeeperArgs {
            /**
             * optional access credentials string with `user:password` format used when use digest authorization in Zookeeper
             */
            identity?: pulumi.Input<string>;
            /**
             * describe every available zookeeper cluster node for interaction
             */
            nodes?: pulumi.Input<pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationTemplateSpecConfigurationZookeeperNodesArgs>[]>;
            /**
             * one operation timeout during Zookeeper transactions
             */
            operation_timeout_ms?: pulumi.Input<number>;
            /**
             * optional root znode path inside zookeeper to store ClickHouse related data (replication queue or distributed DDL)
             */
            root?: pulumi.Input<string>;
            /**
             * session timeout during connect to Zookeeper
             */
            session_timeout_ms?: pulumi.Input<number>;
        }

        export interface ClickHouseInstallationTemplateSpecConfigurationZookeeperNodesArgs {
            /**
             * dns name or ip address for Zookeeper node
             */
            host?: pulumi.Input<string>;
            /**
             * TCP port which used to connect to Zookeeper node
             */
            port?: pulumi.Input<number>;
            /**
             * if a secure connection to Zookeeper is required
             */
            secure?: pulumi.Input<string>;
        }

        /**
         * define default behavior for whole ClickHouseInstallation, some behavior can be re-define on cluster, shard and replica level
         * More info: https://github.com/Altinity/clickhouse-operator/blob/master/docs/custom_resource_explained.md#specdefaults
         */
        export interface ClickHouseInstallationTemplateSpecDefaultsArgs {
            /**
             * allows change `<yandex><distributed_ddl></distributed_ddl></yandex>` settings
             * More info: https://clickhouse.tech/docs/en/operations/server-configuration-parameters/settings/#server-settings-distributed_ddl
             */
            distributedDDL?: pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationTemplateSpecDefaultsDistributedddlArgs>;
            /**
             * define should replicas be specified by FQDN in `<host></host>`.
             * In case of "no" will use short hostname and clickhouse-server will use kubernetes default suffixes for DNS lookup
             * "yes" by default
             */
            replicasUseFQDN?: pulumi.Input<string>;
            /**
             * default storage management options
             */
            storageManagement?: pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationTemplateSpecDefaultsStoragemanagementArgs>;
            /**
             * optional, configuration of the templates names which will use for generate Kubernetes resources according to one or more ClickHouse clusters described in current ClickHouseInstallation (chi) resource
             */
            templates?: pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationTemplateSpecDefaultsTemplatesArgs>;
        }

        /**
         * allows change `<yandex><distributed_ddl></distributed_ddl></yandex>` settings
         * More info: https://clickhouse.tech/docs/en/operations/server-configuration-parameters/settings/#server-settings-distributed_ddl
         */
        export interface ClickHouseInstallationTemplateSpecDefaultsDistributedddlArgs {
            /**
             * Settings from this profile will be used to execute DDL queries
             */
            profile?: pulumi.Input<string>;
        }

        /**
         * default storage management options
         */
        export interface ClickHouseInstallationTemplateSpecDefaultsStoragemanagementArgs {
            /**
             * defines `PVC` provisioner - be it StatefulSet or the Operator
             */
            provisioner?: pulumi.Input<string>;
            /**
             * defines behavior of `PVC` deletion.
             * `Delete` by default, if `Retain` specified then `PVC` will be kept when deleting StatefulSet
             */
            reclaimPolicy?: pulumi.Input<string>;
        }

        /**
         * optional, configuration of the templates names which will use for generate Kubernetes resources according to one or more ClickHouse clusters described in current ClickHouseInstallation (chi) resource
         */
        export interface ClickHouseInstallationTemplateSpecDefaultsTemplatesArgs {
            /**
             * optional, template name from chi.spec.templates.serviceTemplates, allows customization for each `Service` resource which will created by `clickhouse-operator` which cover each clickhouse cluster described in `chi.spec.configuration.clusters`
             */
            clusterServiceTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.volumeClaimTemplates, allows customization each `PVC` which will mount for clickhouse data directory in each `Pod` during render and reconcile every StatefulSet.spec resource described in `chi.spec.configuration.clusters`
             */
            dataVolumeClaimTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.hostTemplates, which will apply to configure every `clickhouse-server` instance during render ConfigMap resources which will mount into `Pod`
             */
            hostTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.volumeClaimTemplates, allows customization each `PVC` which will mount for clickhouse log directory in each `Pod` during render and reconcile every StatefulSet.spec resource described in `chi.spec.configuration.clusters`
             */
            logVolumeClaimTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.podTemplates, allows customization each `Pod` resource during render and reconcile each StatefulSet.spec resource described in `chi.spec.configuration.clusters`
             */
            podTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.serviceTemplates, allows customization for each `Service` resource which will created by `clickhouse-operator` which cover each replica inside each shard inside each clickhouse cluster described in `chi.spec.configuration.clusters`
             */
            replicaServiceTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.serviceTemplates, allows customization for one `Service` resource which will created by `clickhouse-operator` which cover all clusters in whole `chi` resource
             */
            serviceTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.serviceTemplates, allows customization for each `Service` resource which will created by `clickhouse-operator` which cover each shard inside clickhouse cluster described in `chi.spec.configuration.clusters`
             */
            shardServiceTemplate?: pulumi.Input<string>;
            /**
             * DEPRECATED! VolumeClaimTemplate is deprecated in favor of DataVolumeClaimTemplate and LogVolumeClaimTemplate
             */
            volumeClaimTemplate?: pulumi.Input<string>;
        }

        /**
         * optional, allows tuning reconciling cycle for ClickhouseInstallation from clickhouse-operator side
         */
        export interface ClickHouseInstallationTemplateSpecReconcilingArgs {
            /**
             * optional, define behavior for cleanup Kubernetes resources during reconcile cycle
             */
            cleanup?: pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationTemplateSpecReconcilingCleanupArgs>;
            /**
             * timeout in seconds when `clickhouse-operator` will wait when applied `ConfigMap` during reconcile `ClickhouseInstallation` pods will updated from cache
             * see details: https://kubernetes.io/docs/concepts/configuration/configmap/#mounted-configmaps-are-updated-automatically
             */
            configMapPropagationTimeout?: pulumi.Input<number>;
            /**
             * DEPRECATED
             */
            policy?: pulumi.Input<string>;
        }

        /**
         * optional, define behavior for cleanup Kubernetes resources during reconcile cycle
         */
        export interface ClickHouseInstallationTemplateSpecReconcilingCleanupArgs {
            /**
             * what clickhouse-operator shall do when reconciling Kubernetes resources are failed, default behavior is `Retain`
             */
            reconcileFailedObjects?: pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationTemplateSpecReconcilingCleanupReconcilefailedobjectsArgs>;
            /**
             * what clickhouse-operator shall do when found Kubernetes resources which should be managed with clickhouse-operator, but not have `ownerReference` to any currently managed `ClickHouseInstallation` resource, default behavior is `Delete`
             */
            unknownObjects?: pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationTemplateSpecReconcilingCleanupUnknownobjectsArgs>;
        }

        /**
         * what clickhouse-operator shall do when reconciling Kubernetes resources are failed, default behavior is `Retain`
         */
        export interface ClickHouseInstallationTemplateSpecReconcilingCleanupReconcilefailedobjectsArgs {
            /**
             * behavior policy for failed ConfigMap reconciling, Retain by default
             */
            configMap?: pulumi.Input<string>;
            /**
             * behavior policy for failed PVC reconciling, Retain by default
             */
            pvc?: pulumi.Input<string>;
            /**
             * behavior policy for failed Service reconciling, Retain by default
             */
            service?: pulumi.Input<string>;
            /**
             * behavior policy for failed StatefulSet reconciling, Retain by default
             */
            statefulSet?: pulumi.Input<string>;
        }

        /**
         * what clickhouse-operator shall do when found Kubernetes resources which should be managed with clickhouse-operator, but not have `ownerReference` to any currently managed `ClickHouseInstallation` resource, default behavior is `Delete`
         */
        export interface ClickHouseInstallationTemplateSpecReconcilingCleanupUnknownobjectsArgs {
            /**
             * behavior policy for unknown ConfigMap, Delete by default
             */
            configMap?: pulumi.Input<string>;
            /**
             * behavior policy for unknown PVC, Delete by default
             */
            pvc?: pulumi.Input<string>;
            /**
             * behavior policy for unknown Service, Delete by default
             */
            service?: pulumi.Input<string>;
            /**
             * behavior policy for unknown StatefulSet, Delete by default
             */
            statefulSet?: pulumi.Input<string>;
        }

        /**
         * allows define templates which will use for render Kubernetes resources like StatefulSet, ConfigMap, Service, PVC, by default, clickhouse-operator have own templates, but you can override it
         */
        export interface ClickHouseInstallationTemplateSpecTemplatesArgs {
            /**
             * hostTemplate will use during apply to generate `clickhose-server` config files
             */
            hostTemplates?: pulumi.Input<pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationTemplateSpecTemplatesHosttemplatesArgs>[]>;
            /**
             * podTemplate will use during render `Pod` inside `StatefulSet.spec` and allows define rendered `Pod.spec`, pod scheduling distribution and pod zone
             * More information: https://github.com/Altinity/clickhouse-operator/blob/master/docs/custom_resource_explained.md#spectemplatespodtemplates
             */
            podTemplates?: pulumi.Input<pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationTemplateSpecTemplatesPodtemplatesArgs>[]>;
            /**
             * allows define template for rendering `Service` which would get endpoint from Pods which scoped chi-wide, cluster-wide, shard-wide, replica-wide level
             */
            serviceTemplates?: pulumi.Input<pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationTemplateSpecTemplatesServicetemplatesArgs>[]>;
            /**
             * allows define template for rendering `PVC` kubernetes resource, which would use inside `Pod` for mount clickhouse `data`, clickhouse `logs` or something else
             */
            volumeClaimTemplates?: pulumi.Input<pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationTemplateSpecTemplatesVolumeclaimtemplatesArgs>[]>;
        }

        export interface ClickHouseInstallationTemplateSpecTemplatesHosttemplatesArgs {
            /**
             * template name, could use to link inside top-level `chi.spec.defaults.templates.hostTemplate`, cluster-level `chi.spec.configuration.clusters.templates.hostTemplate`, shard-level `chi.spec.configuration.clusters.layout.shards.temlates.hostTemplate`, replica-level `chi.spec.configuration.clusters.layout.replicas.templates.hostTemplate`
             */
            name?: pulumi.Input<string>;
            /**
             * define how will distribute numeric values of named ports in `Pod.spec.containers.ports` and clickhouse-server configs
             */
            portDistribution?: pulumi.Input<pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationTemplateSpecTemplatesHosttemplatesPortdistributionArgs>[]>;
            spec?: pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationTemplateSpecTemplatesHosttemplatesSpecArgs>;
        }

        export interface ClickHouseInstallationTemplateSpecTemplatesHosttemplatesPortdistributionArgs {
            /**
             * type of distribution, when `Unspecified` (default value) then all listen ports on clickhouse-server configuration in all Pods will have the same value, when `ClusterScopeIndex` then ports will increment to offset from base value depends on shard and replica index inside cluster with combination of `chi.spec.templates.podTemlates.spec.HostNetwork` it allows setup ClickHouse cluster inside Kubernetes and provide access via external network bypass Kubernetes internal network
             */
            type?: pulumi.Input<string>;
        }

        export interface ClickHouseInstallationTemplateSpecTemplatesHosttemplatesSpecArgs {
            /**
             * optional, allows define content of any setting file inside each `Pod` where this template will apply during generate `ConfigMap` which will mount in `/etc/clickhouse-server/config.d/` or `/etc/clickhouse-server/conf.d/` or `/etc/clickhouse-server/users.d/`
             */
            files?: pulumi.Input<{[key: string]: any}>;
            /**
             * optional, setup `http_port` inside `clickhouse-server` settings for each Pod where current template will apply
             * if specified, should have equal value with `chi.spec.templates.podTemplates.spec.containers.ports[name=http]`
             * More info: https://clickhouse.tech/docs/en/interfaces/http/
             */
            httpPort?: pulumi.Input<number>;
            httpsPort?: pulumi.Input<number>;
            /**
             * optional, open insecure ports for cluster, defaults to "yes"
             */
            insecure?: pulumi.Input<string>;
            /**
             * optional, setup `interserver_http_port` inside `clickhouse-server` settings for each Pod where current template will apply
             * if specified, should have equal value with `chi.spec.templates.podTemplates.spec.containers.ports[name=interserver]`
             * More info: https://clickhouse.tech/docs/en/operations/server-configuration-parameters/settings/#interserver-http-port
             */
            interserverHTTPPort?: pulumi.Input<number>;
            /**
             * by default, hostname will generate, but this allows define custom name for each `clickhuse-server`
             */
            name?: pulumi.Input<string>;
            /**
             * optional, open secure ports
             */
            secure?: pulumi.Input<string>;
            /**
             * optional, allows configure `clickhouse-server` settings inside <yandex>...</yandex> tag in each `Pod` where this template will apply during generate `ConfigMap` which will mount in `/etc/clickhouse-server/conf.d/`
             * More details: https://clickhouse.tech/docs/en/operations/settings/settings/
             */
            settings?: pulumi.Input<{[key: string]: any}>;
            /**
             * optional, setup `tcp_port` inside `clickhouse-server` settings for each Pod where current template will apply
             * if specified, should have equal value with `chi.spec.templates.podTemplates.spec.containers.ports[name=tcp]`
             * More info: https://clickhouse.tech/docs/en/interfaces/tcp/
             */
            tcpPort?: pulumi.Input<number>;
            /**
             * be careful, this part of CRD allows override template inside template, don't use it if you don't understand what you do
             */
            templates?: pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationTemplateSpecTemplatesHosttemplatesSpecTemplatesArgs>;
            tlsPort?: pulumi.Input<number>;
        }

        /**
         * be careful, this part of CRD allows override template inside template, don't use it if you don't understand what you do
         */
        export interface ClickHouseInstallationTemplateSpecTemplatesHosttemplatesSpecTemplatesArgs {
            /**
             * optional, template name from chi.spec.templates.serviceTemplates, allows customization for each `Service` resource which will created by `clickhouse-operator` which cover each clickhouse cluster described in `chi.spec.configuration.clusters`
             */
            clusterServiceTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.volumeClaimTemplates, allows customization each `PVC` which will mount for clickhouse data directory in each `Pod` during render and reconcile every StatefulSet.spec resource described in `chi.spec.configuration.clusters`
             */
            dataVolumeClaimTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.hostTemplates, which will apply to configure every `clickhouse-server` instance during render ConfigMap resources which will mount into `Pod`
             */
            hostTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.volumeClaimTemplates, allows customization each `PVC` which will mount for clickhouse log directory in each `Pod` during render and reconcile every StatefulSet.spec resource described in `chi.spec.configuration.clusters`
             */
            logVolumeClaimTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.podTemplates, allows customization each `Pod` resource during render and reconcile each StatefulSet.spec resource described in `chi.spec.configuration.clusters`
             */
            podTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.serviceTemplates, allows customization for each `Service` resource which will created by `clickhouse-operator` which cover each replica inside each shard inside each clickhouse cluster described in `chi.spec.configuration.clusters`
             */
            replicaServiceTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.serviceTemplates, allows customization for one `Service` resource which will created by `clickhouse-operator` which cover all clusters in whole `chi` resource
             */
            serviceTemplate?: pulumi.Input<string>;
            /**
             * optional, template name from chi.spec.templates.serviceTemplates, allows customization for each `Service` resource which will created by `clickhouse-operator` which cover each shard inside clickhouse cluster described in `chi.spec.configuration.clusters`
             */
            shardServiceTemplate?: pulumi.Input<string>;
            /**
             * DEPRECATED! VolumeClaimTemplate is deprecated in favor of DataVolumeClaimTemplate and LogVolumeClaimTemplate
             */
            volumeClaimTemplate?: pulumi.Input<string>;
        }

        export interface ClickHouseInstallationTemplateSpecTemplatesPodtemplatesArgs {
            /**
             * DEPRECATED, shortcut for `chi.spec.templates.podTemplates.spec.affinity.podAntiAffinity`
             */
            distribution?: pulumi.Input<string>;
            /**
             * allows define format for generated `Pod` name, look to https://github.com/Altinity/clickhouse-operator/blob/master/docs/custom_resource_explained.md#spectemplatesservicetemplates for details about aviailable template variables
             */
            generateName?: pulumi.Input<string>;
            /**
             * allows pass standard object's metadata from template to Pod
             * More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata
             */
            metadata?: pulumi.Input<{[key: string]: any}>;
            /**
             * template name, could use to link inside top-level `chi.spec.defaults.templates.podTemplate`, cluster-level `chi.spec.configuration.clusters.templates.podTemplate`, shard-level `chi.spec.configuration.clusters.layout.shards.temlates.podTemplate`, replica-level `chi.spec.configuration.clusters.layout.replicas.templates.podTemplate`
             */
            name?: pulumi.Input<string>;
            /**
             * define ClickHouse Pod distribution policy between Kubernetes Nodes inside Shard, Replica, Namespace, CHI, another ClickHouse cluster
             */
            podDistribution?: pulumi.Input<pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationTemplateSpecTemplatesPodtemplatesPoddistributionArgs>[]>;
            /**
             * allows define whole Pod.spec inside StaefulSet.spec, look to https://kubernetes.io/docs/concepts/workloads/pods/#pod-templates for details
             */
            spec?: pulumi.Input<{[key: string]: any}>;
            /**
             * allows define custom zone name and will separate ClickHouse `Pods` between nodes, shortcut for `chi.spec.templates.podTemplates.spec.affinity.podAntiAffinity`
             */
            zone?: pulumi.Input<inputs.clickhouse.v1.ClickHouseInstallationTemplateSpecTemplatesPodtemplatesZoneArgs>;
        }

        export interface ClickHouseInstallationTemplateSpecTemplatesPodtemplatesPoddistributionArgs {
            /**
             * define, how much ClickHouse Pods could be inside selected scope with selected distribution type
             */
            number?: pulumi.Input<number>;
            /**
             * scope for apply each podDistribution
             */
            scope?: pulumi.Input<string>;
            /**
             * use for inter-pod affinity look to `pod.spec.affinity.podAntiAffinity.preferredDuringSchedulingIgnoredDuringExecution.podAffinityTerm.topologyKey`, More info: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
             */
            topologyKey?: pulumi.Input<string>;
            /**
             * you can define multiple affinity policy types
             */
            type?: pulumi.Input<string>;
        }

        /**
         * allows define custom zone name and will separate ClickHouse `Pods` between nodes, shortcut for `chi.spec.templates.podTemplates.spec.affinity.podAntiAffinity`
         */
        export interface ClickHouseInstallationTemplateSpecTemplatesPodtemplatesZoneArgs {
            /**
             * optional, if defined, allows select kubernetes nodes by label with `name` equal `key`
             */
            key?: pulumi.Input<string>;
            /**
             * optional, if defined, allows select kubernetes nodes by label with `value` in `values`
             */
            values?: pulumi.Input<pulumi.Input<string>[]>;
        }

        export interface ClickHouseInstallationTemplateSpecTemplatesServicetemplatesArgs {
            /**
             * allows define format for generated `Service` name, look to https://github.com/Altinity/clickhouse-operator/blob/master/docs/custom_resource_explained.md#spectemplatesservicetemplates for details about aviailable template variables
             */
            generateName?: pulumi.Input<string>;
            /**
             * allows pass standard object's metadata from template to Service
             * Could be use for define specificly for Cloud Provider metadata which impact to behavior of service
             * More info: https://kubernetes.io/docs/concepts/services-networking/service/
             */
            metadata?: pulumi.Input<{[key: string]: any}>;
            /**
             * template name, could use to link inside
             * chi-level `chi.spec.defaults.templates.serviceTemplate`
             * cluster-level `chi.spec.configuration.clusters.templates.clusterServiceTemplate`
             * shard-level `chi.spec.configuration.clusters.layout.shards.temlates.shardServiceTemplate`
             * replica-level `chi.spec.configuration.clusters.layout.replicas.templates.replicaServiceTemplate` or `chi.spec.configuration.clusters.layout.shards.replicas.replicaServiceTemplate`
             */
            name?: pulumi.Input<string>;
            /**
             * describe behavior of generated Service
             * More info: https://kubernetes.io/docs/concepts/services-networking/service/
             */
            spec?: pulumi.Input<{[key: string]: any}>;
        }

        export interface ClickHouseInstallationTemplateSpecTemplatesVolumeclaimtemplatesArgs {
            /**
             * allows to pass standard object's metadata from template to PVC
             * More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata
             */
            metadata?: pulumi.Input<{[key: string]: any}>;
            /**
             * template name, could use to link inside
             * top-level `chi.spec.defaults.templates.dataVolumeClaimTemplate` or `chi.spec.defaults.templates.logVolumeClaimTemplate`,
             * cluster-level `chi.spec.configuration.clusters.templates.dataVolumeClaimTemplate` or `chi.spec.configuration.clusters.templates.logVolumeClaimTemplate`,
             * shard-level `chi.spec.configuration.clusters.layout.shards.temlates.dataVolumeClaimTemplate` or `chi.spec.configuration.clusters.layout.shards.temlates.logVolumeClaimTemplate`
             * replica-level `chi.spec.configuration.clusters.layout.replicas.templates.dataVolumeClaimTemplate` or `chi.spec.configuration.clusters.layout.replicas.templates.logVolumeClaimTemplate`
             */
            name?: pulumi.Input<string>;
            /**
             * defines `PVC` provisioner - be it StatefulSet or the Operator
             */
            provisioner?: pulumi.Input<string>;
            /**
             * defines behavior of `PVC` deletion.
             * `Delete` by default, if `Retain` specified then `PVC` will be kept when deleting StatefulSet
             */
            reclaimPolicy?: pulumi.Input<string>;
            /**
             * allows define all aspects of `PVC` resource
             * More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims
             */
            spec?: pulumi.Input<{[key: string]: any}>;
        }

        /**
         * optional, define policy for auto applying ClickHouseInstallationTemplate inside ClickHouseInstallation
         */
        export interface ClickHouseInstallationTemplateSpecTemplatingArgs {
            /**
             * when defined as `auto` inside ClickhouseInstallationTemplate, it will auto add into all ClickHouseInstallation, manual value is default
             */
            policy?: pulumi.Input<string>;
        }

        export interface ClickHouseInstallationTemplateSpecUsetemplatesArgs {
            /**
             * name of `ClickHouseInstallationTemplate` (chit) resource
             */
            name?: pulumi.Input<string>;
            /**
             * Kubernetes namespace where need search `chit` resource, depending on `watchNamespaces` settings in `clichouse-operator`
             */
            namespace?: pulumi.Input<string>;
            /**
             * optional, current strategy is only merge, and current `chi` settings have more priority than merged template `chit`
             */
            useType?: pulumi.Input<string>;
        }

        /**
         * Current ClickHouseInstallation manifest status, contains many fields like a normalized configuration, clickhouse-operator version, current action and all applied action list, current taskID and all applied taskIDs and other
         */
        export interface ClickHouseInstallationTemplateStatusArgs {
            /**
             * Action
             */
            action?: pulumi.Input<string>;
            /**
             * Actions
             */
            actions?: pulumi.Input<pulumi.Input<string>[]>;
            /**
             * ClickHouse operator git commit SHA
             */
            chopCommit?: pulumi.Input<string>;
            /**
             * ClickHouse operator build date
             */
            chopDate?: pulumi.Input<string>;
            /**
             * IP address of the operator's pod which managed this CHI
             */
            chopIp?: pulumi.Input<string>;
            /**
             * ClickHouse operator version
             */
            chopVersion?: pulumi.Input<string>;
            /**
             * Clusters count
             */
            clusters?: pulumi.Input<number>;
            /**
             * Endpoint
             */
            endpoint?: pulumi.Input<string>;
            /**
             * Last error
             */
            error?: pulumi.Input<string>;
            /**
             * Errors
             */
            errors?: pulumi.Input<pulumi.Input<string>[]>;
            /**
             * Pods FQDNs
             */
            fqdns?: pulumi.Input<pulumi.Input<string>[]>;
            /**
             * Generation
             */
            generation?: pulumi.Input<number>;
            /**
             * Hosts count
             */
            hosts?: pulumi.Input<number>;
            /**
             * Added Hosts count
             */
            hostsAdded?: pulumi.Input<number>;
            /**
             * Completed Hosts count
             */
            hostsCompleted?: pulumi.Input<number>;
            /**
             * About to delete Hosts count
             */
            hostsDelete?: pulumi.Input<number>;
            /**
             * Deleted Hosts count
             */
            hostsDeleted?: pulumi.Input<number>;
            /**
             * Updated Hosts count
             */
            hostsUpdated?: pulumi.Input<number>;
            /**
             * List of hosts with tables created by the operator
             */
            hostsWithTablesCreated?: pulumi.Input<pulumi.Input<string>[]>;
            /**
             * Normalized CHI requested
             */
            normalized?: pulumi.Input<{[key: string]: any}>;
            /**
             * Normalized CHI completed
             */
            normalizedCompleted?: pulumi.Input<{[key: string]: any}>;
            /**
             * Pod IPs
             */
            podIps?: pulumi.Input<pulumi.Input<string>[]>;
            /**
             * Pods
             */
            pods?: pulumi.Input<pulumi.Input<string>[]>;
            /**
             * Replicas count
             */
            replicas?: pulumi.Input<number>;
            /**
             * Shards count
             */
            shards?: pulumi.Input<number>;
            /**
             * Status
             */
            status?: pulumi.Input<string>;
            /**
             * Current task id
             */
            taskID?: pulumi.Input<string>;
            /**
             * Completed task ids
             */
            taskIDsCompleted?: pulumi.Input<pulumi.Input<string>[]>;
            /**
             * Started task ids
             */
            taskIDsStarted?: pulumi.Input<pulumi.Input<string>[]>;
        }

    }
}

export namespace metallb {
    export namespace v1alpha1 {
        /**
         * AddressPoolSpec defines the desired state of AddressPool.
         */
        export interface AddressPoolSpecArgs {
            /**
             * A list of IP address ranges over which MetalLB has authority. You can list multiple ranges in a single pool, they will all share the same settings. Each range can be either a CIDR prefix, or an explicit start-end range of IPs.
             */
            addresses: pulumi.Input<pulumi.Input<string>[]>;
            /**
             * AutoAssign flag used to prevent MetallB from automatic allocation for a pool.
             */
            autoAssign?: pulumi.Input<boolean>;
            /**
             * When an IP is allocated from this pool, how should it be translated into BGP announcements?
             */
            bgpAdvertisements?: pulumi.Input<pulumi.Input<inputs.metallb.v1alpha1.AddressPoolSpecBgpadvertisementsArgs>[]>;
            /**
             * Protocol can be used to select how the announcement is done.
             */
            protocol: pulumi.Input<string>;
        }
        /**
         * addressPoolSpecArgsProvideDefaults sets the appropriate defaults for AddressPoolSpecArgs
         */
        export function addressPoolSpecArgsProvideDefaults(val: AddressPoolSpecArgs): AddressPoolSpecArgs {
            return {
                ...val,
                autoAssign: (val.autoAssign) ?? true,
            };
        }

        export interface AddressPoolSpecBgpadvertisementsArgs {
            /**
             * The aggregation-length advertisement option lets you “roll up” the /32s into a larger prefix.
             */
            aggregationLength?: pulumi.Input<number>;
            /**
             * Optional, defaults to 128 (i.e. no aggregation) if not specified.
             */
            aggregationLengthV6?: pulumi.Input<number>;
            /**
             * BGP communities
             */
            communities?: pulumi.Input<pulumi.Input<string>[]>;
            /**
             * BGP LOCAL_PREF attribute which is used by BGP best path algorithm, Path with higher localpref is preferred over one with lower localpref.
             */
            localPref?: pulumi.Input<number>;
        }
        /**
         * addressPoolSpecBgpadvertisementsArgsProvideDefaults sets the appropriate defaults for AddressPoolSpecBgpadvertisementsArgs
         */
        export function addressPoolSpecBgpadvertisementsArgsProvideDefaults(val: AddressPoolSpecBgpadvertisementsArgs): AddressPoolSpecBgpadvertisementsArgs {
            return {
                ...val,
                aggregationLength: (val.aggregationLength) ?? 32,
                aggregationLengthV6: (val.aggregationLengthV6) ?? 128,
            };
        }

    }

    export namespace v1beta1 {
        /**
         * AddressPoolSpec defines the desired state of AddressPool.
         */
        export interface AddressPoolSpecArgs {
            /**
             * A list of IP address ranges over which MetalLB has authority. You can list multiple ranges in a single pool, they will all share the same settings. Each range can be either a CIDR prefix, or an explicit start-end range of IPs.
             */
            addresses: pulumi.Input<pulumi.Input<string>[]>;
            /**
             * AutoAssign flag used to prevent MetallB from automatic allocation for a pool.
             */
            autoAssign?: pulumi.Input<boolean>;
            /**
             * Drives how an IP allocated from this pool should translated into BGP announcements.
             */
            bgpAdvertisements?: pulumi.Input<pulumi.Input<inputs.metallb.v1beta1.AddressPoolSpecBgpadvertisementsArgs>[]>;
            /**
             * Protocol can be used to select how the announcement is done.
             */
            protocol: pulumi.Input<string>;
        }
        /**
         * addressPoolSpecArgsProvideDefaults sets the appropriate defaults for AddressPoolSpecArgs
         */
        export function addressPoolSpecArgsProvideDefaults(val: AddressPoolSpecArgs): AddressPoolSpecArgs {
            return {
                ...val,
                autoAssign: (val.autoAssign) ?? true,
            };
        }

        export interface AddressPoolSpecBgpadvertisementsArgs {
            /**
             * The aggregation-length advertisement option lets you “roll up” the /32s into a larger prefix.
             */
            aggregationLength?: pulumi.Input<number>;
            /**
             * Optional, defaults to 128 (i.e. no aggregation) if not specified.
             */
            aggregationLengthV6?: pulumi.Input<number>;
            /**
             * BGP communities to be associated with the given advertisement.
             */
            communities?: pulumi.Input<pulumi.Input<string>[]>;
            /**
             * BGP LOCAL_PREF attribute which is used by BGP best path algorithm, Path with higher localpref is preferred over one with lower localpref.
             */
            localPref?: pulumi.Input<number>;
        }
        /**
         * addressPoolSpecBgpadvertisementsArgsProvideDefaults sets the appropriate defaults for AddressPoolSpecBgpadvertisementsArgs
         */
        export function addressPoolSpecBgpadvertisementsArgsProvideDefaults(val: AddressPoolSpecBgpadvertisementsArgs): AddressPoolSpecBgpadvertisementsArgs {
            return {
                ...val,
                aggregationLength: (val.aggregationLength) ?? 32,
                aggregationLengthV6: (val.aggregationLengthV6) ?? 128,
            };
        }

        /**
         * BFDProfileSpec defines the desired state of BFDProfile.
         */
        export interface BFDProfileSpecArgs {
            /**
             * Configures the detection multiplier to determine packet loss. The remote transmission interval will be multiplied by this value to determine the connection loss detection timer.
             */
            detectMultiplier?: pulumi.Input<number>;
            /**
             * Configures the minimal echo receive transmission interval that this system is capable of handling in milliseconds. Defaults to 50ms
             */
            echoInterval?: pulumi.Input<number>;
            /**
             * Enables or disables the echo transmission mode. This mode is disabled by default, and not supported on multi hops setups.
             */
            echoMode?: pulumi.Input<boolean>;
            /**
             * For multi hop sessions only: configure the minimum expected TTL for an incoming BFD control packet.
             */
            minimumTtl?: pulumi.Input<number>;
            /**
             * Mark session as passive: a passive session will not attempt to start the connection and will wait for control packets from peer before it begins replying.
             */
            passiveMode?: pulumi.Input<boolean>;
            /**
             * The minimum interval that this system is capable of receiving control packets in milliseconds. Defaults to 300ms.
             */
            receiveInterval?: pulumi.Input<number>;
            /**
             * The minimum transmission interval (less jitter) that this system wants to use to send BFD control packets in milliseconds. Defaults to 300ms
             */
            transmitInterval?: pulumi.Input<number>;
        }

        /**
         * BGPAdvertisementSpec defines the desired state of BGPAdvertisement.
         */
        export interface BGPAdvertisementSpecArgs {
            /**
             * The aggregation-length advertisement option lets you “roll up” the /32s into a larger prefix. Defaults to 32. Works for IPv4 addresses.
             */
            aggregationLength?: pulumi.Input<number>;
            /**
             * The aggregation-length advertisement option lets you “roll up” the /128s into a larger prefix. Defaults to 128. Works for IPv6 addresses.
             */
            aggregationLengthV6?: pulumi.Input<number>;
            /**
             * The BGP communities to be associated with the announcement. Each item can be a community of the form 1234:1234 or the name of an alias defined in the Community CRD.
             */
            communities?: pulumi.Input<pulumi.Input<string>[]>;
            /**
             * A selector for the IPAddressPools which would get advertised via this advertisement. If no IPAddressPool is selected by this or by the list, the advertisement is applied to all the IPAddressPools.
             */
            ipAddressPoolSelectors?: pulumi.Input<pulumi.Input<inputs.metallb.v1beta1.BGPAdvertisementSpecIpaddresspoolselectorsArgs>[]>;
            /**
             * The list of IPAddressPools to advertise via this advertisement, selected by name.
             */
            ipAddressPools?: pulumi.Input<pulumi.Input<string>[]>;
            /**
             * The BGP LOCAL_PREF attribute which is used by BGP best path algorithm, Path with higher localpref is preferred over one with lower localpref.
             */
            localPref?: pulumi.Input<number>;
            /**
             * NodeSelectors allows to limit the nodes to announce as next hops for the LoadBalancer IP. When empty, all the nodes having  are announced as next hops.
             */
            nodeSelectors?: pulumi.Input<pulumi.Input<inputs.metallb.v1beta1.BGPAdvertisementSpecNodeselectorsArgs>[]>;
            /**
             * Peers limits the bgppeer to advertise the ips of the selected pools to. When empty, the loadbalancer IP is announced to all the BGPPeers configured.
             */
            peers?: pulumi.Input<pulumi.Input<string>[]>;
        }
        /**
         * bgpadvertisementSpecArgsProvideDefaults sets the appropriate defaults for BGPAdvertisementSpecArgs
         */
        export function bgpadvertisementSpecArgsProvideDefaults(val: BGPAdvertisementSpecArgs): BGPAdvertisementSpecArgs {
            return {
                ...val,
                aggregationLength: (val.aggregationLength) ?? 32,
                aggregationLengthV6: (val.aggregationLengthV6) ?? 128,
            };
        }

        /**
         * A label selector is a label query over a set of resources. The result of matchLabels and matchExpressions are ANDed. An empty label selector matches all objects. A null label selector matches no objects.
         */
        export interface BGPAdvertisementSpecIpaddresspoolselectorsArgs {
            /**
             * matchExpressions is a list of label selector requirements. The requirements are ANDed.
             */
            matchExpressions?: pulumi.Input<pulumi.Input<inputs.metallb.v1beta1.BGPAdvertisementSpecIpaddresspoolselectorsMatchexpressionsArgs>[]>;
            /**
             * matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is "key", the operator is "In", and the values array contains only "value". The requirements are ANDed.
             */
            matchLabels?: pulumi.Input<{[key: string]: pulumi.Input<string>}>;
        }

        /**
         * A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values.
         */
        export interface BGPAdvertisementSpecIpaddresspoolselectorsMatchexpressionsArgs {
            /**
             * key is the label key that the selector applies to.
             */
            key: pulumi.Input<string>;
            /**
             * operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist.
             */
            operator: pulumi.Input<string>;
            /**
             * values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch.
             */
            values?: pulumi.Input<pulumi.Input<string>[]>;
        }

        /**
         * A label selector is a label query over a set of resources. The result of matchLabels and matchExpressions are ANDed. An empty label selector matches all objects. A null label selector matches no objects.
         */
        export interface BGPAdvertisementSpecNodeselectorsArgs {
            /**
             * matchExpressions is a list of label selector requirements. The requirements are ANDed.
             */
            matchExpressions?: pulumi.Input<pulumi.Input<inputs.metallb.v1beta1.BGPAdvertisementSpecNodeselectorsMatchexpressionsArgs>[]>;
            /**
             * matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is "key", the operator is "In", and the values array contains only "value". The requirements are ANDed.
             */
            matchLabels?: pulumi.Input<{[key: string]: pulumi.Input<string>}>;
        }

        /**
         * A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values.
         */
        export interface BGPAdvertisementSpecNodeselectorsMatchexpressionsArgs {
            /**
             * key is the label key that the selector applies to.
             */
            key: pulumi.Input<string>;
            /**
             * operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist.
             */
            operator: pulumi.Input<string>;
            /**
             * values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch.
             */
            values?: pulumi.Input<pulumi.Input<string>[]>;
        }

        /**
         * BGPPeerSpec defines the desired state of Peer.
         */
        export interface BGPPeerSpecArgs {
            bfdProfile?: pulumi.Input<string>;
            /**
             * EBGP peer is multi-hops away
             */
            ebgpMultiHop?: pulumi.Input<boolean>;
            /**
             * Requested BGP hold time, per RFC4271.
             */
            holdTime?: pulumi.Input<string>;
            /**
             * Requested BGP keepalive time, per RFC4271.
             */
            keepaliveTime?: pulumi.Input<string>;
            /**
             * AS number to use for the local end of the session.
             */
            myASN: pulumi.Input<number>;
            /**
             * Only connect to this peer on nodes that match one of these selectors.
             */
            nodeSelectors?: pulumi.Input<pulumi.Input<inputs.metallb.v1beta1.BGPPeerSpecNodeselectorsArgs>[]>;
            /**
             * Authentication password for routers enforcing TCP MD5 authenticated sessions
             */
            password?: pulumi.Input<string>;
            /**
             * AS number to expect from the remote end of the session.
             */
            peerASN: pulumi.Input<number>;
            /**
             * Address to dial when establishing the session.
             */
            peerAddress: pulumi.Input<string>;
            /**
             * Port to dial when establishing the session.
             */
            peerPort?: pulumi.Input<number>;
            /**
             * BGP router ID to advertise to the peer
             */
            routerID?: pulumi.Input<string>;
            /**
             * Source address to use when establishing the session.
             */
            sourceAddress?: pulumi.Input<string>;
        }

        export interface BGPPeerSpecNodeselectorsArgs {
            matchExpressions?: pulumi.Input<pulumi.Input<inputs.metallb.v1beta1.BGPPeerSpecNodeselectorsMatchexpressionsArgs>[]>;
            matchLabels?: pulumi.Input<{[key: string]: pulumi.Input<string>}>;
        }

        export interface BGPPeerSpecNodeselectorsMatchexpressionsArgs {
            key: pulumi.Input<string>;
            operator: pulumi.Input<string>;
            values: pulumi.Input<pulumi.Input<string>[]>;
        }

        /**
         * CommunitySpec defines the desired state of Community.
         */
        export interface CommunitySpecArgs {
            communities?: pulumi.Input<pulumi.Input<inputs.metallb.v1beta1.CommunitySpecCommunitiesArgs>[]>;
        }

        export interface CommunitySpecCommunitiesArgs {
            /**
             * The name of the alias for the community.
             */
            name?: pulumi.Input<string>;
            /**
             * The BGP community value corresponding to the given name.
             */
            value?: pulumi.Input<string>;
        }

        /**
         * IPAddressPoolSpec defines the desired state of IPAddressPool.
         */
        export interface IPAddressPoolSpecArgs {
            /**
             * A list of IP address ranges over which MetalLB has authority. You can list multiple ranges in a single pool, they will all share the same settings. Each range can be either a CIDR prefix, or an explicit start-end range of IPs.
             */
            addresses: pulumi.Input<pulumi.Input<string>[]>;
            /**
             * AutoAssign flag used to prevent MetallB from automatic allocation for a pool.
             */
            autoAssign?: pulumi.Input<boolean>;
            /**
             * AvoidBuggyIPs prevents addresses ending with .0 and .255 to be used by a pool.
             */
            avoidBuggyIPs?: pulumi.Input<boolean>;
            /**
             * AllocateTo makes ip pool allocation to specific namespace and/or service. The controller will use the pool with lowest value of priority in case of multiple matches. A pool with no priority set will be used only if the pools with priority can't be used. If multiple matching IPAddressPools are available it will check for the availability of IPs sorting the matching IPAddressPools by priority, starting from the highest to the lowest. If multiple IPAddressPools have the same priority, choice will be random.
             */
            serviceAllocation?: pulumi.Input<inputs.metallb.v1beta1.IPAddressPoolSpecServiceallocationArgs>;
        }
        /**
         * ipaddressPoolSpecArgsProvideDefaults sets the appropriate defaults for IPAddressPoolSpecArgs
         */
        export function ipaddressPoolSpecArgsProvideDefaults(val: IPAddressPoolSpecArgs): IPAddressPoolSpecArgs {
            return {
                ...val,
                autoAssign: (val.autoAssign) ?? true,
                avoidBuggyIPs: (val.avoidBuggyIPs) ?? false,
            };
        }

        /**
         * AllocateTo makes ip pool allocation to specific namespace and/or service. The controller will use the pool with lowest value of priority in case of multiple matches. A pool with no priority set will be used only if the pools with priority can't be used. If multiple matching IPAddressPools are available it will check for the availability of IPs sorting the matching IPAddressPools by priority, starting from the highest to the lowest. If multiple IPAddressPools have the same priority, choice will be random.
         */
        export interface IPAddressPoolSpecServiceallocationArgs {
            /**
             * NamespaceSelectors list of label selectors to select namespace(s) for ip pool, an alternative to using namespace list.
             */
            namespaceSelectors?: pulumi.Input<pulumi.Input<inputs.metallb.v1beta1.IPAddressPoolSpecServiceallocationNamespaceselectorsArgs>[]>;
            /**
             * Namespaces list of namespace(s) on which ip pool can be attached.
             */
            namespaces?: pulumi.Input<pulumi.Input<string>[]>;
            /**
             * Priority priority given for ip pool while ip allocation on a service.
             */
            priority?: pulumi.Input<number>;
            /**
             * ServiceSelectors list of label selector to select service(s) for which ip pool can be used for ip allocation.
             */
            serviceSelectors?: pulumi.Input<pulumi.Input<inputs.metallb.v1beta1.IPAddressPoolSpecServiceallocationServiceselectorsArgs>[]>;
        }

        /**
         * A label selector is a label query over a set of resources. The result of matchLabels and matchExpressions are ANDed. An empty label selector matches all objects. A null label selector matches no objects.
         */
        export interface IPAddressPoolSpecServiceallocationNamespaceselectorsArgs {
            /**
             * matchExpressions is a list of label selector requirements. The requirements are ANDed.
             */
            matchExpressions?: pulumi.Input<pulumi.Input<inputs.metallb.v1beta1.IPAddressPoolSpecServiceallocationNamespaceselectorsMatchexpressionsArgs>[]>;
            /**
             * matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is "key", the operator is "In", and the values array contains only "value". The requirements are ANDed.
             */
            matchLabels?: pulumi.Input<{[key: string]: pulumi.Input<string>}>;
        }

        /**
         * A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values.
         */
        export interface IPAddressPoolSpecServiceallocationNamespaceselectorsMatchexpressionsArgs {
            /**
             * key is the label key that the selector applies to.
             */
            key: pulumi.Input<string>;
            /**
             * operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist.
             */
            operator: pulumi.Input<string>;
            /**
             * values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch.
             */
            values?: pulumi.Input<pulumi.Input<string>[]>;
        }

        /**
         * A label selector is a label query over a set of resources. The result of matchLabels and matchExpressions are ANDed. An empty label selector matches all objects. A null label selector matches no objects.
         */
        export interface IPAddressPoolSpecServiceallocationServiceselectorsArgs {
            /**
             * matchExpressions is a list of label selector requirements. The requirements are ANDed.
             */
            matchExpressions?: pulumi.Input<pulumi.Input<inputs.metallb.v1beta1.IPAddressPoolSpecServiceallocationServiceselectorsMatchexpressionsArgs>[]>;
            /**
             * matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is "key", the operator is "In", and the values array contains only "value". The requirements are ANDed.
             */
            matchLabels?: pulumi.Input<{[key: string]: pulumi.Input<string>}>;
        }

        /**
         * A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values.
         */
        export interface IPAddressPoolSpecServiceallocationServiceselectorsMatchexpressionsArgs {
            /**
             * key is the label key that the selector applies to.
             */
            key: pulumi.Input<string>;
            /**
             * operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist.
             */
            operator: pulumi.Input<string>;
            /**
             * values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch.
             */
            values?: pulumi.Input<pulumi.Input<string>[]>;
        }

        /**
         * L2AdvertisementSpec defines the desired state of L2Advertisement.
         */
        export interface L2AdvertisementSpecArgs {
            /**
             * A list of interfaces to announce from. The LB IP will be announced only from these interfaces. If the field is not set, we advertise from all the interfaces on the host.
             */
            interfaces?: pulumi.Input<pulumi.Input<string>[]>;
            /**
             * A selector for the IPAddressPools which would get advertised via this advertisement. If no IPAddressPool is selected by this or by the list, the advertisement is applied to all the IPAddressPools.
             */
            ipAddressPoolSelectors?: pulumi.Input<pulumi.Input<inputs.metallb.v1beta1.L2AdvertisementSpecIpaddresspoolselectorsArgs>[]>;
            /**
             * The list of IPAddressPools to advertise via this advertisement, selected by name.
             */
            ipAddressPools?: pulumi.Input<pulumi.Input<string>[]>;
            /**
             * NodeSelectors allows to limit the nodes to announce as next hops for the LoadBalancer IP. When empty, all the nodes having  are announced as next hops.
             */
            nodeSelectors?: pulumi.Input<pulumi.Input<inputs.metallb.v1beta1.L2AdvertisementSpecNodeselectorsArgs>[]>;
        }

        /**
         * A label selector is a label query over a set of resources. The result of matchLabels and matchExpressions are ANDed. An empty label selector matches all objects. A null label selector matches no objects.
         */
        export interface L2AdvertisementSpecIpaddresspoolselectorsArgs {
            /**
             * matchExpressions is a list of label selector requirements. The requirements are ANDed.
             */
            matchExpressions?: pulumi.Input<pulumi.Input<inputs.metallb.v1beta1.L2AdvertisementSpecIpaddresspoolselectorsMatchexpressionsArgs>[]>;
            /**
             * matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is "key", the operator is "In", and the values array contains only "value". The requirements are ANDed.
             */
            matchLabels?: pulumi.Input<{[key: string]: pulumi.Input<string>}>;
        }

        /**
         * A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values.
         */
        export interface L2AdvertisementSpecIpaddresspoolselectorsMatchexpressionsArgs {
            /**
             * key is the label key that the selector applies to.
             */
            key: pulumi.Input<string>;
            /**
             * operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist.
             */
            operator: pulumi.Input<string>;
            /**
             * values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch.
             */
            values?: pulumi.Input<pulumi.Input<string>[]>;
        }

        /**
         * A label selector is a label query over a set of resources. The result of matchLabels and matchExpressions are ANDed. An empty label selector matches all objects. A null label selector matches no objects.
         */
        export interface L2AdvertisementSpecNodeselectorsArgs {
            /**
             * matchExpressions is a list of label selector requirements. The requirements are ANDed.
             */
            matchExpressions?: pulumi.Input<pulumi.Input<inputs.metallb.v1beta1.L2AdvertisementSpecNodeselectorsMatchexpressionsArgs>[]>;
            /**
             * matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is "key", the operator is "In", and the values array contains only "value". The requirements are ANDed.
             */
            matchLabels?: pulumi.Input<{[key: string]: pulumi.Input<string>}>;
        }

        /**
         * A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values.
         */
        export interface L2AdvertisementSpecNodeselectorsMatchexpressionsArgs {
            /**
             * key is the label key that the selector applies to.
             */
            key: pulumi.Input<string>;
            /**
             * operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist.
             */
            operator: pulumi.Input<string>;
            /**
             * values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch.
             */
            values?: pulumi.Input<pulumi.Input<string>[]>;
        }

    }

    export namespace v1beta2 {
        /**
         * BGPPeerSpec defines the desired state of Peer.
         */
        export interface BGPPeerSpecArgs {
            /**
             * The name of the BFD Profile to be used for the BFD session associated to the BGP session. If not set, the BFD session won't be set up.
             */
            bfdProfile?: pulumi.Input<string>;
            /**
             * To set if the BGPPeer is multi-hops away. Needed for FRR mode only.
             */
            ebgpMultiHop?: pulumi.Input<boolean>;
            /**
             * Requested BGP hold time, per RFC4271.
             */
            holdTime?: pulumi.Input<string>;
            /**
             * Requested BGP keepalive time, per RFC4271.
             */
            keepaliveTime?: pulumi.Input<string>;
            /**
             * AS number to use for the local end of the session.
             */
            myASN: pulumi.Input<number>;
            /**
             * Only connect to this peer on nodes that match one of these selectors.
             */
            nodeSelectors?: pulumi.Input<pulumi.Input<inputs.metallb.v1beta2.BGPPeerSpecNodeselectorsArgs>[]>;
            /**
             * Authentication password for routers enforcing TCP MD5 authenticated sessions
             */
            password?: pulumi.Input<string>;
            /**
             * passwordSecret is name of the authentication secret for BGP Peer. the secret must be of type "kubernetes.io/basic-auth", and created in the same namespace as the MetalLB deployment. The password is stored in the secret as the key "password".
             */
            passwordSecret?: pulumi.Input<inputs.metallb.v1beta2.BGPPeerSpecPasswordsecretArgs>;
            /**
             * AS number to expect from the remote end of the session.
             */
            peerASN: pulumi.Input<number>;
            /**
             * Address to dial when establishing the session.
             */
            peerAddress: pulumi.Input<string>;
            /**
             * Port to dial when establishing the session.
             */
            peerPort?: pulumi.Input<number>;
            /**
             * BGP router ID to advertise to the peer
             */
            routerID?: pulumi.Input<string>;
            /**
             * Source address to use when establishing the session.
             */
            sourceAddress?: pulumi.Input<string>;
            /**
             * To set if we want to peer with the BGPPeer using an interface belonging to a host vrf
             */
            vrf?: pulumi.Input<string>;
        }
        /**
         * bgppeerSpecArgsProvideDefaults sets the appropriate defaults for BGPPeerSpecArgs
         */
        export function bgppeerSpecArgsProvideDefaults(val: BGPPeerSpecArgs): BGPPeerSpecArgs {
            return {
                ...val,
                peerPort: (val.peerPort) ?? 179,
            };
        }

        /**
         * A label selector is a label query over a set of resources. The result of matchLabels and matchExpressions are ANDed. An empty label selector matches all objects. A null label selector matches no objects.
         */
        export interface BGPPeerSpecNodeselectorsArgs {
            /**
             * matchExpressions is a list of label selector requirements. The requirements are ANDed.
             */
            matchExpressions?: pulumi.Input<pulumi.Input<inputs.metallb.v1beta2.BGPPeerSpecNodeselectorsMatchexpressionsArgs>[]>;
            /**
             * matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to an element of matchExpressions, whose key field is "key", the operator is "In", and the values array contains only "value". The requirements are ANDed.
             */
            matchLabels?: pulumi.Input<{[key: string]: pulumi.Input<string>}>;
        }

        /**
         * A label selector requirement is a selector that contains values, a key, and an operator that relates the key and values.
         */
        export interface BGPPeerSpecNodeselectorsMatchexpressionsArgs {
            /**
             * key is the label key that the selector applies to.
             */
            key: pulumi.Input<string>;
            /**
             * operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and DoesNotExist.
             */
            operator: pulumi.Input<string>;
            /**
             * values is an array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during a strategic merge patch.
             */
            values?: pulumi.Input<pulumi.Input<string>[]>;
        }

        /**
         * passwordSecret is name of the authentication secret for BGP Peer. the secret must be of type "kubernetes.io/basic-auth", and created in the same namespace as the MetalLB deployment. The password is stored in the secret as the key "password".
         */
        export interface BGPPeerSpecPasswordsecretArgs {
            /**
             * Name is unique within a namespace to reference a secret resource.
             */
            name?: pulumi.Input<string>;
            /**
             * Namespace defines the space within which the secret name must be unique.
             */
            namespace?: pulumi.Input<string>;
        }

    }
}
